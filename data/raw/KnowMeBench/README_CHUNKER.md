# KnowMeBench é€šç”¨è¯­ä¹‰åˆ†å—å·¥å…·ä½¿ç”¨è¯´æ˜Ž

## ðŸ“– æ¦‚è¿°

`semantic_chunker_universal.py` æ˜¯ä¸€ä¸ªé€šç”¨çš„è¯­ä¹‰åˆ†å—å·¥å…·ï¼Œæ”¯æŒ KnowMeBench çš„æ‰€æœ‰ä¸‰ä¸ªæ•°æ®é›†ï¼ˆdataset1, dataset2, dataset3ï¼‰ã€‚

## ðŸ†š ä¸‰ä¸ªDatasetçš„å·®å¼‚

| ç‰¹æ€§ | Dataset1 | Dataset2 | Dataset3 |
|------|----------|----------|----------|
| **å†…å®¹å­—æ®µ** | `inner_thought` | `mind` | `Mind` (å¤§å†™) |
| **çŽ¯å¢ƒå­—æ®µ** | `environment` | `environment` | `Environment` (å¤§å†™) |
| **èƒŒæ™¯å­—æ®µ** | `background` | `background` | `Background` (å¤§å†™) |
| **è®°å½•æ•°é‡** | 6,644 | 11,995 | 8,423 |
| **é¢å¤–å­—æ®µ** | - | `category` | `category` |

## ðŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•

```bash
# å¤„ç†å•ä¸ªæ•°æ®é›†
python3 semantic_chunker_universal.py --dataset dataset1

# å¤„ç†æ‰€æœ‰æ•°æ®é›†
python3 semantic_chunker_universal.py --dataset all
```

### å®Œæ•´å‚æ•°

```bash
python3 semantic_chunker_universal.py \
  --dataset dataset1 \              # æ•°æ®é›†é€‰æ‹©: dataset1, dataset2, dataset3, all
  --input-dir ./KnowmeBench \       # è¾“å…¥ç›®å½•è·¯å¾„
  --output-dir ./chunked_output \   # è¾“å‡ºç›®å½•è·¯å¾„
  --min-tokens 3000 \               # æœ€å°chunkå¤§å°ï¼ˆtokensï¼‰
  --max-tokens 6000                 # æœ€å¤§chunkå¤§å°ï¼ˆtokensï¼‰
```

## ðŸ“Š å¤„ç†ç»“æžœ

### Dataset1
- **è®°å½•æ•°**: 6,644
- **ç”Ÿæˆchunks**: 240
- **å¹³å‡å¤§å°**: 3,624 tokens
- **åœ°ç‚¹è¦†ç›–**: 1,496 ä¸ª

### Dataset2
- **è®°å½•æ•°**: 11,995
- **ç”Ÿæˆchunks**: 256
- **å¹³å‡å¤§å°**: 3,620 tokens
- **åœ°ç‚¹è¦†ç›–**: 627 ä¸ª

### Dataset3
- **è®°å½•æ•°**: 8,423
- **ç”Ÿæˆchunks**: 219
- **å¹³å‡å¤§å°**: 3,678 tokens
- **åœ°ç‚¹è¦†ç›–**: 402 ä¸ª

## ðŸ“ è¾“å‡ºæ–‡ä»¶

æ¯ä¸ªæ•°æ®é›†ä¼šç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶ï¼š

```
chunked_output/
â”œâ”€â”€ dataset1_chunks.json          # JSONæ ¼å¼ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
â”œâ”€â”€ dataset1_chunks_text.txt      # çº¯æ–‡æœ¬æ ¼å¼ï¼ˆä¾¿äºŽé˜…è¯»ï¼‰
â”œâ”€â”€ dataset2_chunks.json
â”œâ”€â”€ dataset2_chunks_text.txt
â”œâ”€â”€ dataset3_chunks.json
â””â”€â”€ dataset3_chunks_text.txt
```

### JSONæ ¼å¼ç¤ºä¾‹

```json
[
  {
    "chunk_id": 0,
    "text": "[æ—¶é—´æˆ³] åœ°ç‚¹ | å†…å®¹...",
    "start_id": 0,
    "end_id": 25,
    "record_count": 26,
    "token_count": 3456,
    "start_time": "1969-08-15 14:00:00",
    "end_time": "1969-08-16 10:30:00",
    "locations": ["åœ°ç‚¹1", "åœ°ç‚¹2", ...]
  },
  ...
]
```

## ðŸ” æ ¸å¿ƒç‰¹æ€§

### 1. è‡ªé€‚åº”å­—æ®µæ˜ å°„
- è‡ªåŠ¨è¯†åˆ«ä¸åŒæ•°æ®é›†çš„å­—æ®µåå·®å¼‚
- Dataset3 çš„å¤§å†™å­—æ®µï¼ˆ`Environment`, `Background`, `Mind`ï¼‰è‡ªåŠ¨é€‚é…

### 2. æ™ºèƒ½è¯­ä¹‰åˆ†å—
- åŸºäºŽæ—¶é—´è·³è·ƒã€åœ°ç‚¹å˜åŒ–ã€å†…å®¹å¯†åº¦ç­‰å¤šç»´åº¦æ£€æµ‹è¯­ä¹‰è¾¹ç•Œ
- åŠ¨æ€è°ƒæ•´åˆ†å—å¤§å°ï¼ˆ3k-6k tokensï¼‰

### 3. å†…å®¹ä¼˜å…ˆçº§
æ‰€æœ‰æ•°æ®é›†æŒ‰ç›¸åŒä¼˜å…ˆçº§ç»„åˆå†…å®¹ï¼š
1. `action` - è¡ŒåŠ¨
2. `dialogue` - å¯¹è¯
3. `environment/Environment` - çŽ¯å¢ƒ
4. `background/Background` - èƒŒæ™¯
5. `inner_thought/mind/Mind` - å†…å¿ƒæƒ³æ³•

## ðŸ› ï¸ æŠ€æœ¯ç»†èŠ‚

### Tokenä¼°ç®—ç®—æ³•
```python
tokens = (
    è‹±æ–‡å­—ç¬¦ / 4.0 +
    ä¸­æ–‡å­—ç¬¦ / 2.0 +
    æ•°å­— / 3.0 +
    ç¬¦å· / 2.0 +
    ç©ºæ ¼ / 1.5
) Ã— 1.1  # +10%ç¼“å†²
```

### è¯­ä¹‰è¾¹ç•Œæ£€æµ‹
```
è¾¹ç•Œå¼ºåº¦ = æ—¶é—´è·³è·ƒå¼ºåº¦ + åœ°ç‚¹å˜åŒ–å¼ºåº¦ + å†…å®¹å¯†åº¦å˜åŒ– + å†…å®¹é•¿åº¦å¼‚å¸¸

é˜ˆå€¼ = 0.5 (å¯è°ƒæ•´)
```

## ðŸ’¡ ä½¿ç”¨å»ºè®®

1. **é¦–æ¬¡ä½¿ç”¨**ï¼šå…ˆå¤„ç†å•ä¸ªæ•°æ®é›†éªŒè¯æ•ˆæžœ
   ```bash
   python3 semantic_chunker_universal.py --dataset dataset1
   ```

2. **æ‰¹é‡å¤„ç†**ï¼šéªŒè¯æ— è¯¯åŽæ‰¹é‡å¤„ç†
   ```bash
   python3 semantic_chunker_universal.py --dataset all
   ```

3. **è°ƒæ•´å‚æ•°**ï¼šæ ¹æ®å®žé™…éœ€æ±‚è°ƒæ•´tokenèŒƒå›´
   ```bash
   python3 semantic_chunker_universal.py --dataset all --min-tokens 2000 --max-tokens 5000
   ```

## ðŸ“ ä¸ŽåŽŸè„šæœ¬çš„åŒºåˆ«

| ç‰¹æ€§ | åŽŸè„šæœ¬ (semantic_chunker.py) | æ–°è„šæœ¬ (semantic_chunker_universal.py) |
|------|------------------------------|----------------------------------------|
| **æ”¯æŒæ•°æ®é›†** | ä»… dataset1 | dataset1, dataset2, dataset3 |
| **å­—æ®µé€‚é…** | ç¡¬ç¼–ç  | è‡ªåŠ¨é€‚é… |
| **å‘½ä»¤è¡Œå‚æ•°** | æ—  | å®Œæ•´CLIæ”¯æŒ |
| **æ‰¹é‡å¤„ç†** | ä¸æ”¯æŒ | æ”¯æŒ `--dataset all` |
| **è¾“å‡ºå‘½å** | å›ºå®š | æŒ‰æ•°æ®é›†åç§°åŒºåˆ† |

## âš™ï¸ é«˜çº§é…ç½®

### å­—æ®µæ˜ å°„é…ç½®
å¦‚æžœéœ€è¦æ·»åŠ æ–°çš„æ•°æ®é›†ï¼Œåªéœ€åœ¨ `FIELD_MAPPINGS` ä¸­æ·»åŠ é…ç½®ï¼š

```python
FIELD_MAPPINGS = {
    'dataset4': {
        'content_fields': ['action', 'dialogue', 'thought'],
        'timestamp': 'time',
        'location': 'place'
    }
}
```

### è¾¹ç•Œæ£€æµ‹å‚æ•°
```python
chunker = UniversalSemanticChunker(
    min_tokens=3000,          # æœ€å°chunkå¤§å°
    max_tokens=6000,          # æœ€å¤§chunkå¤§å°
    overlap_tokens=200,       # é‡å ä¿æŠ¤
    boundary_threshold=0.5,   # è¯­ä¹‰è¾¹ç•Œé˜ˆå€¼
    dataset_type='dataset1'
)
```

## ðŸ› å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆæœ‰äº›chunkå°äºŽ3000 tokensï¼Ÿ
A: æœ€åŽä¸€ä¸ªchunkå¯èƒ½ä¸æ»¡è¶³æœ€å°å¤§å°è¦æ±‚ï¼Œæˆ–è€…æ˜¯ç”±äºŽè¯­ä¹‰è¾¹ç•Œå¼ºåˆ¶åˆ†å‰²ã€‚

### Q: å¦‚ä½•è°ƒæ•´chunkå¤§å°åˆ†å¸ƒï¼Ÿ
A: ä¿®æ”¹ `--min-tokens` å’Œ `--max-tokens` å‚æ•°ï¼Œæˆ–è°ƒæ•´ `boundary_threshold`ã€‚

### Q: Dataset3çš„å¤§å†™å­—æ®µæ˜¯å¦æ­£ç¡®å¤„ç†ï¼Ÿ
A: æ˜¯çš„ï¼Œè„šæœ¬å·²è‡ªåŠ¨é€‚é… `Environment`, `Background`, `Mind` è¿™äº›å¤§å†™å­—æ®µã€‚

## ðŸ“ž æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. è¾“å…¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
2. æ•°æ®æ ¼å¼æ˜¯å¦ç¬¦åˆé¢„æœŸ
3. Pythonç‰ˆæœ¬ >= 3.7

---

**ä½œè€…**: Claude
**æ›´æ–°æ—¶é—´**: 2026-02-09
**ç‰ˆæœ¬**: 2.0 - Universal Edition
