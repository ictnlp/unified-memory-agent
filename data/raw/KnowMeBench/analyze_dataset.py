#!/usr/bin/env python3
"""
KnowMeBenché€šç”¨æ•°æ®é›†åˆ†æè„šæœ¬
æ”¯æŒdataset1, dataset2, dataset3çš„è‡ªé€‚åº”åˆ†æ
"""

import json
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional, Set
import statistics
from collections import Counter
from pathlib import Path


class UniversalDatasetAnalyzer:
    """KnowMeBenché€šç”¨æ•°æ®é›†åˆ†æå™¨"""

    # å®šä¹‰ä¸‰ä¸ªdatasetçš„å­—æ®µæ˜ å°„
    FIELD_MAPPINGS = {
        'dataset1': {
            'content_fields': ['action', 'dialogue', 'environment', 'background', 'inner_thought'],
            'timestamp': 'timestamp',
            'location': 'location',
            'id': 'id'
        },
        'dataset2': {
            'content_fields': ['action', 'dialogue', 'environment', 'background', 'mind'],
            'timestamp': 'timestamp',
            'location': 'location',
            'id': 'id'
        },
        'dataset3': {
            'content_fields': ['action', 'dialogue', 'Environment', 'Background', 'Mind'],
            'timestamp': 'timestamp',
            'location': 'location',
            'id': 'id'
        }
    }

    def __init__(self, dataset_path: str, dataset_type: str = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            dataset_path: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
            dataset_type: æ•°æ®é›†ç±»å‹ (auto, dataset1, dataset2, dataset3)
        """
        self.dataset_path = dataset_path
        self.data = []
        self.analysis_results = {}
        self.dataset_type = dataset_type
        self.field_config = None

        # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹
        if dataset_type == 'auto' or dataset_type is None:
            self.dataset_type = self._detect_dataset_type()
        else:
            if dataset_type not in self.FIELD_MAPPINGS:
                raise ValueError(f"ä¸æ”¯æŒçš„datasetç±»å‹: {dataset_type}. æ”¯æŒçš„ç±»å‹: {list(self.FIELD_MAPPINGS.keys())}")

        self.field_config = self.FIELD_MAPPINGS[self.dataset_type]
        print(f"æ£€æµ‹åˆ°æ•°æ®é›†ç±»å‹: {self.dataset_type}")
        print(f"å†…å®¹å­—æ®µ: {self.field_config['content_fields']}")

    def _detect_dataset_type(self) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹"""
        # å…ˆåŠ è½½æ•°æ®
        print(f"åŠ è½½æ•°æ®é›†: {self.dataset_path}")
        with open(self.dataset_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        if not self.data:
            raise ValueError("æ•°æ®é›†ä¸ºç©º")

        # æ£€æŸ¥ç¬¬ä¸€æ¡è®°å½•çš„å­—æ®µ
        first_record = self.data[0]
        fields = set(first_record.keys())

        # æ ¹æ®å­—æ®µç‰¹å¾åˆ¤æ–­
        if 'inner_thought' in fields:
            return 'dataset1'
        elif 'mind' in fields and 'Mind' not in fields:
            return 'dataset2'
        elif 'Mind' in fields:
            return 'dataset3'
        else:
            # é»˜è®¤è¿”å›dataset1
            print("æ— æ³•è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹ï¼Œé»˜è®¤ä½¿ç”¨dataset1")
            return 'dataset1'

    def load_dataset(self) -> None:
        """åŠ è½½JSONæ•°æ®é›†"""
        if not self.data:
            print(f"åŠ è½½æ•°æ®é›†: {self.dataset_path}")
            with open(self.dataset_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
        print(f"æˆåŠŸåŠ è½½ {len(self.data)} æ¡è®°å½•")

    def analyze_basic_info(self) -> Dict[str, Any]:
        """åˆ†æåŸºç¡€ä¿¡æ¯"""
        if not self.data:
            raise ValueError("æ•°æ®æœªåŠ è½½")

        total_records = len(self.data)

        # è·å–æ‰€æœ‰å­—æ®µï¼ˆåŒ…æ‹¬é¢å¤–å­—æ®µå¦‚categoryï¼‰
        all_fields = set()
        for record in self.data:
            all_fields.update(record.keys())

        # æ ‡å‡†å­—æ®µ + å†…å®¹å­—æ®µ
        fields = [
            self.field_config.get('id', 'id'),
            self.field_config.get('timestamp', 'timestamp'),
            self.field_config.get('location', 'location')
        ]
        fields.extend(self.field_config['content_fields'])

        # æ·»åŠ é¢å¤–å­—æ®µï¼ˆå¦‚categoryï¼‰
        extra_fields = list(all_fields - set(fields))
        fields.extend(extra_fields)

        # æ—¶é—´èŒƒå›´
        timestamps = []
        for record in self.data:
            ts = record.get(self.field_config.get('timestamp', 'timestamp'))
            if ts:
                try:
                    timestamps.append(datetime.strptime(ts, '%Y-%m-%d %H:%M:%S'))
                except (ValueError, TypeError):
                    pass

        timestamps.sort()
        time_span_days = (timestamps[-1] - timestamps[0]).days if timestamps else 0

        # åœ°ç‚¹ç»Ÿè®¡
        location_field = self.field_config.get('location', 'location')
        locations = [record.get(location_field) for record in self.data if record.get(location_field)]
        unique_locations = len(set(locations))

        # å­—æ®µå®Œæ•´æ€§ç»Ÿè®¡
        field_completeness = {}
        for field in fields:
            non_null_count = sum(1 for record in self.data if record.get(field) is not None)
            completeness = (non_null_count / total_records) * 100
            field_completeness[field] = {
                'count': non_null_count,
                'percentage': round(completeness, 1)
            }

        basic_info = {
            'dataset_type': self.dataset_type,
            'total_records': total_records,
            'all_fields': sorted(list(all_fields)),
            'time_span_years': round(time_span_days / 365.25, 1),
            'unique_locations': unique_locations,
            'time_range': {
                'start': timestamps[0].strftime('%Y-%m-%d %H:%M:%S') if timestamps else None,
                'end': timestamps[-1].strftime('%Y-%m-%d %H:%M:%S') if timestamps else None
            },
            'field_completeness': field_completeness,
            'content_fields': self.field_config['content_fields']
        }

        return basic_info

    def analyze_time_distribution(self) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´åˆ†å¸ƒ"""
        time_intervals = []
        prev_timestamp = None
        timestamp_field = self.field_config.get('timestamp', 'timestamp')

        for record in self.data:
            ts = record.get(timestamp_field)
            if ts:
                try:
                    current_time = datetime.strptime(ts, '%Y-%m-%d %H:%M:%S')
                    if prev_timestamp:
                        interval_seconds = (current_time - prev_timestamp).total_seconds()
                        time_intervals.append(interval_seconds)
                    prev_timestamp = current_time
                except (ValueError, TypeError):
                    pass

        if not time_intervals:
            return {'error': 'æ²¡æœ‰æœ‰æ•ˆçš„æ—¶é—´æˆ³æ•°æ®'}

        time_intervals_minutes = [interval / 60 for interval in time_intervals]

        # ç»Ÿè®¡ä¸åŒæ—¶é—´é—´éš”çš„æ•°é‡
        interval_counts = Counter()
        for interval in time_intervals_minutes:
            if interval <= 1:
                interval_counts['<=1åˆ†é’Ÿ'] += 1
            elif interval <= 5:
                interval_counts['1-5åˆ†é’Ÿ'] += 1
            elif interval <= 30:
                interval_counts['5-30åˆ†é’Ÿ'] += 1
            elif interval <= 60:
                interval_counts['30-60åˆ†é’Ÿ'] += 1
            elif interval <= 1440:  # 24å°æ—¶
                interval_counts['1-24å°æ—¶'] += 1
            elif interval <= 10080:  # 7å¤©
                interval_counts['1-7å¤©'] += 1
            else:
                interval_counts['>7å¤©'] += 1

        time_stats = {
            'total_intervals': len(time_intervals),
            'avg_interval_minutes': round(statistics.mean(time_intervals_minutes), 2),
            'median_interval_minutes': round(statistics.median(time_intervals_minutes), 2),
            'min_interval_minutes': round(min(time_intervals_minutes), 2),
            'max_interval_minutes': round(max(time_intervals_minutes), 2),
            'interval_distribution': dict(interval_counts)
        }

        return time_stats

    def analyze_content_length(self) -> Dict[str, Any]:
        """åˆ†æå„å­—æ®µå†…å®¹é•¿åº¦"""
        field_content_lengths = {}

        for field in self.field_config['content_fields']:
            lengths = []
            for record in self.data:
                content = record.get(field)
                if content and isinstance(content, str):
                    lengths.append(len(content))

            if lengths:
                field_content_lengths[field] = {
                    'avg_length': round(statistics.mean(lengths), 2),
                    'median_length': round(statistics.median(lengths), 2),
                    'min_length': min(lengths),
                    'max_length': max(lengths),
                    'non_empty_records': len(lengths)
                }
            else:
                field_content_lengths[field] = {
                    'error': 'æ— æœ‰æ•ˆæ•°æ®'
                }

        return field_content_lengths

    def analyze_location_distribution(self) -> Dict[str, Any]:
        """åˆ†æåœ°ç‚¹åˆ†å¸ƒ"""
        location_field = self.field_config.get('location', 'location')

        location_counts = Counter()
        location_with_time = {}  # è®°å½•åœ°ç‚¹é¦–æ¬¡å‡ºç°çš„æ—¶é—´

        for record in self.data:
            location = record.get(location_field)
            if location:
                location_counts[location] += 1
                if location not in location_with_time and record.get(self.field_config.get('timestamp', 'timestamp')):
                    location_with_time[location] = record.get(self.field_config.get('timestamp', 'timestamp'))

        top_locations = location_counts.most_common(20)

        location_stats = {
            'unique_locations': len(location_counts),
            'location_frequency': dict(location_counts),
            'top_20_locations': top_locations,
            'location_first_appearance': location_with_time
        }

        return location_stats

    def analyze_category_distribution(self) -> Dict[str, Any]:
        """åˆ†æcategoryå­—æ®µåˆ†å¸ƒï¼ˆå¦‚æœæœ‰ï¼‰"""
        category_field = 'category'

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨categoryå­—æ®µ
        has_category = any('category' in record for record in self.data)

        if not has_category:
            return {'note': 'è¯¥æ•°æ®é›†æ²¡æœ‰categoryå­—æ®µ'}

        # æ”¶é›†æ‰€æœ‰category
        categories = []
        category_sets = []
        for record in self.data:
            cat = record.get(category_field)
            if cat:
                categories.append(cat)
                # å°è¯•è§£æä¸ºåˆ—è¡¨ï¼ˆå¦‚ "background, mind"ï¼‰
                if isinstance(cat, str):
                    cat_list = [c.strip() for c in cat.split(',')]
                    category_sets.append(set(cat_list))

        if not categories:
            return {'note': 'categoryå­—æ®µæ²¡æœ‰æœ‰æ•ˆæ•°æ®'}

        # ç»Ÿè®¡categoryé¢‘ç‡
        category_counts = Counter(categories)

        # ç»Ÿè®¡categoryç»„åˆ
        unique_combinations = []
        if category_sets:
            seen_combinations = set()
            for cat_set in category_sets:
                combo = ', '.join(sorted(cat_set))
                if combo not in seen_combinations:
                    seen_combinations.add(combo)
                    unique_combinations.append(combo)

        category_stats = {
            'total_with_category': len(categories),
            'unique_categories': len(category_counts),
            'category_frequency': dict(category_counts),
            'unique_combinations': unique_combinations,
            'sample_categories': categories[:10]
        }

        return category_stats

    def analyze_temporal_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        timestamp_field = self.field_config.get('timestamp', 'timestamp')

        # æŒ‰å°æ—¶ã€å¤©ã€æœˆã€å¹´ç»Ÿè®¡
        hours = []
        days = []
        months = []
        years = []

        for record in self.data:
            ts = record.get(timestamp_field)
            if ts:
                try:
                    dt = datetime.strptime(ts, '%Y-%m-%d %H:%M:%S')
                    hours.append(dt.hour)
                    days.append(dt.weekday())  # 0=å‘¨ä¸€, 6=å‘¨æ—¥
                    months.append(dt.month)
                    years.append(dt.year)
                except (ValueError, TypeError):
                    pass

        weekday_names = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']

        temporal_stats = {
            'hour_distribution': dict(Counter(hours)),
            'weekday_distribution': {weekday_names[d]: count for d, count in Counter(days).items()},
            'month_distribution': dict(Counter(months)),
            'year_distribution': dict(Counter(years)),
            'total_temporal_records': len(hours)
        }

        return temporal_stats

    def analyze_content_cooccurrence(self) -> Dict[str, Any]:
        """åˆ†æå†…å®¹å­—æ®µå…±ç°æƒ…å†µ"""
        cooccurrence = {}
        content_fields = self.field_config['content_fields']

        for record in self.data:
            active_fields = []
            for field in content_fields:
                content = record.get(field)
                if content and isinstance(content, str) and content.strip():
                    active_fields.append(field)

            if len(active_fields) > 1:
                combo = tuple(sorted(active_fields))
                cooccurrence[combo] = cooccurrence.get(combo, 0) + 1

        # æ’åº
        sorted_cooccurrence = sorted(cooccurrence.items(), key=lambda x: x[1], reverse=True)

        return {
            'total_cooccurrences': sum(cooccurrence.values()),
            'top_combinations': sorted_cooccurrence[:20],
            'all_combinations': [f"{' + '.join(combo)}: {count}" for combo, count in sorted_cooccurrence]
        }

    def run_full_analysis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("å¼€å§‹åˆ†ææ•°æ®é›†...")

        # åŠ è½½æ•°æ®
        self.load_dataset()

        # åŸºç¡€ä¿¡æ¯åˆ†æ
        print("åˆ†æåŸºç¡€ä¿¡æ¯...")
        basic_info = self.analyze_basic_info()

        # æ—¶é—´åˆ†å¸ƒåˆ†æ
        print("åˆ†ææ—¶é—´åˆ†å¸ƒ...")
        time_distribution = self.analyze_time_distribution()

        # å†…å®¹é•¿åº¦åˆ†æ
        print("åˆ†æå†…å®¹é•¿åº¦...")
        content_length = self.analyze_content_length()

        # åœ°ç‚¹åˆ†å¸ƒåˆ†æ
        print("åˆ†æåœ°ç‚¹åˆ†å¸ƒ...")
        location_distribution = self.analyze_location_distribution()

        # Categoryåˆ†æï¼ˆå¦‚æœæœ‰ï¼‰
        print("åˆ†æcategoryåˆ†å¸ƒ...")
        category_distribution = self.analyze_category_distribution()

        # æ—¶é—´æ¨¡å¼åˆ†æ
        print("åˆ†ææ—¶é—´æ¨¡å¼...")
        temporal_patterns = self.analyze_temporal_patterns()

        # å†…å®¹å­—æ®µå…±ç°åˆ†æ
        print("åˆ†æå†…å®¹å­—æ®µå…±ç°...")
        content_cooccurrence = self.analyze_content_cooccurrence()

        # æ±‡æ€»ç»“æœ
        analysis_results = {
            'dataset_type': self.dataset_type,
            'basic_info': basic_info,
            'time_distribution': time_distribution,
            'content_length': content_length,
            'location_distribution': location_distribution,
            'category_distribution': category_distribution,
            'temporal_patterns': temporal_patterns,
            'content_cooccurrence': content_cooccurrence
        }

        self.analysis_results = analysis_results
        return analysis_results

    def print_summary(self) -> None:
        """æ‰“å°åˆ†ææ‘˜è¦"""
        if not self.analysis_results:
            print("è¯·å…ˆè¿è¡Œåˆ†æ")
            return

        results = self.analysis_results

        print("\n" + "="*70)
        print(f"KnowMeBenchæ•°æ®é›†åˆ†ææ‘˜è¦ - {results['dataset_type'].upper()}")
        print("="*70)

        # åŸºç¡€ä¿¡æ¯
        basic = results['basic_info']
        print(f"\nğŸ“Š åŸºç¡€ä¿¡æ¯:")
        print(f"  æ•°æ®é›†ç±»å‹: {basic['dataset_type']}")
        print(f"  æ€»è®°å½•æ•°: {basic['total_records']:,}")
        print(f"  æ—¶é—´è·¨åº¦: {basic['time_span_years']} å¹´")
        print(f"  ç‹¬ç‰¹åœ°ç‚¹æ•°: {basic['unique_locations']:,}")
        print(f"  æ‰€æœ‰å­—æ®µ: {', '.join(basic['all_fields'])}")

        print(f"\nğŸ“ˆ å­—æ®µå®Œæ•´æ€§:")
        for field, stats in sorted(basic['field_completeness'].items()):
            count = stats['count']
            percentage = stats['percentage']
            bar = 'â–ˆ' * int(percentage / 5)  # æ¯5%ä¸€ä¸ªæ¡
            print(f"  {field:20s}: {count:6,} ({percentage:5.1f}%) {bar}")

        # æ—¶é—´åˆ†å¸ƒ
        time_dist = results['time_distribution']
        if 'error' not in time_dist:
            print(f"\nâ±ï¸  æ—¶é—´åˆ†å¸ƒ:")
            print(f"  å¹³å‡é—´éš”: {time_dist['avg_interval_minutes']} åˆ†é’Ÿ")
            print(f"  ä¸­ä½æ•°é—´éš”: {time_dist['median_interval_minutes']} åˆ†é’Ÿ")
            print(f"  æœ€å°é—´éš”: {time_dist['min_interval_minutes']} åˆ†é’Ÿ")
            print(f"  æœ€å¤§é—´éš”: {time_dist['max_interval_minutes']} åˆ†é’Ÿ")

            print(f"\n  é—´éš”åˆ†å¸ƒ:")
            for interval, count in time_dist['interval_distribution'].items():
                percentage = (count / time_dist['total_intervals']) * 100
                bar = 'â–ˆ' * int(percentage / 5)
                print(f"    {interval:15s}: {count:5,} ({percentage:5.1f}%) {bar}")

        # æ—¶é—´æ¨¡å¼
        temporal = results['temporal_patterns']
        if temporal['total_temporal_records'] > 0:
            print(f"\nğŸ“… æ—¶é—´æ¨¡å¼:")

            print(f"  æ˜ŸæœŸåˆ†å¸ƒ:")
            for day, count in temporal['weekday_distribution'].items():
                percentage = (count / temporal['total_temporal_records']) * 100
                print(f"    {day}: {count} ({percentage:.1f}%)")

            print(f"  å¹´ä»½åˆ†å¸ƒ:")
            for year, count in sorted(temporal['year_distribution'].items()):
                print(f"    {year}: {count} æ¡è®°å½•")

        # Categoryåˆ†å¸ƒ
        category = results['category_distribution']
        if 'note' not in category:
            print(f"\nğŸ·ï¸  Categoryåˆ†æ:")
            print(f"  æœ‰categoryçš„è®°å½•: {category['total_with_category']:,}")
            print(f"  ç‹¬ç‰¹categoryæ•°: {category['unique_categories']}")
            if category['unique_combinations']:
                print(f"  å¸¸è§categoryç»„åˆ:")
                for combo in category['unique_combinations'][:5]:
                    print(f"    {combo}")

        # å†…å®¹é•¿åº¦
        content_len = results['content_length']
        print(f"\nğŸ“ å†…å®¹é•¿åº¦ç»Ÿè®¡:")
        for field, stats in content_len.items():
            if 'error' not in stats:
                print(f"  {field}:")
                print(f"    éç©ºè®°å½•: {stats['non_empty_records']:,}")
                print(f"    å¹³å‡é•¿åº¦: {stats['avg_length']:.0f} å­—ç¬¦")
                print(f"    ä¸­ä½æ•°é•¿åº¦: {stats['median_length']:.0f} å­—ç¬¦")
                print(f"    é•¿åº¦èŒƒå›´: {stats['min_length']} - {stats['max_length']}")

        # å†…å®¹å­—æ®µå…±ç°
        cooccurrence = results['content_cooccurrence']
        if cooccurrence['total_cooccurrences'] > 0:
            print(f"\nğŸ”— å†…å®¹å­—æ®µå…±ç°:")
            print(f"  æ€»å…±ç°æ¬¡æ•°: {cooccurrence['total_cooccurrences']:,}")
            print(f"  å¸¸è§ç»„åˆ:")
            for combo, count in cooccurrence['top_combinations'][:10]:
                print(f"    {' + '.join(combo)}: {count}")

        # åœ°ç‚¹åˆ†å¸ƒ
        location_dist = results['location_distribution']
        print(f"\nğŸ“ åœ°ç‚¹åˆ†å¸ƒ:")
        print(f"  ç‹¬ç‰¹åœ°ç‚¹: {location_dist['unique_locations']:,}")
        print(f"  æœ€å¸¸è§åœ°ç‚¹ (Top 10):")
        for location, count in location_dist['top_20_locations'][:10]:
            print(f"    {location}: {count}")

        print("\n" + "="*70)

    def save_analysis(self, output_path: str = None) -> None:
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        if not self.analysis_results:
            raise ValueError("æ²¡æœ‰åˆ†æç»“æœå¯ä¿å­˜")

        if output_path is None:
            output_path = f"dataset_analysis_{self.dataset_type}.json"

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False, default=str)

        print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def process_dataset(input_file: str, dataset_type: str = 'auto',
                   output_dir: str = None) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ•°æ®é›†

    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        dataset_type: æ•°æ®é›†ç±»å‹ (auto, dataset1, dataset2, dataset3)
        output_dir: è¾“å‡ºç›®å½•
    """
    # åˆ›å»ºåˆ†æå™¨
    analyzer = UniversalDatasetAnalyzer(input_file, dataset_type)

    try:
        # è¿è¡Œåˆ†æ
        results = analyzer.run_full_analysis()

        # æ‰“å°æ‘˜è¦
        analyzer.print_summary()

        # ä¿å­˜ç»“æœ
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            output_file = output_path / f"dataset_analysis_{analyzer.dataset_type}.json"
            analyzer.save_analysis(str(output_file))
        else:
            analyzer.save_analysis()

        return results

    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise


def compare_datasets(input_dir: str, output_dir: str = None) -> Dict[str, Any]:
    """
    æ¯”è¾ƒæ‰€æœ‰ä¸‰ä¸ªæ•°æ®é›†

    Args:
        input_dir: è¾“å…¥ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
    """
    base_path = Path(input_dir)

    dataset_configs = {
        'dataset1': base_path / 'dataset1/input/dataset1.json',
        'dataset2': base_path / 'dataset2/input/dataset2.json',
        'dataset3': base_path / 'dataset3/input/dataset3.json'
    }

    comparison_results = {}

    for dataset_name, input_file in dataset_configs.items():
        if not input_file.exists():
            print(f"\nâš ï¸  è·³è¿‡ {dataset_name}: æ–‡ä»¶ä¸å­˜åœ¨")
            continue

        print(f"\n{'='*70}")
        print(f"å¤„ç† {dataset_name}")
        print(f"{'='*70}")

        try:
            results = process_dataset(str(input_file), dataset_name, output_dir)
            comparison_results[dataset_name] = results
        except Exception as e:
            print(f"âŒ {dataset_name} å¤„ç†å¤±è´¥: {e}")
            continue

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n" + "="*70)
    print("æ•°æ®é›†å¯¹æ¯”æŠ¥å‘Š")
    print("="*70)

    if comparison_results:
        metric_table = []
        for dataset_name in dataset_configs.keys():
            if dataset_name in comparison_results:
                results = comparison_results[dataset_name]
                basic = results['basic_info']
                content_len = results['content_length']

                # è®¡ç®—æ€»å¹³å‡å†…å®¹é•¿åº¦
                total_avg_length = sum(
                    stats.get('avg_length', 0)
                    for stats in content_len.values()
                    if 'error' not in stats
                )

                metric_table.append({
                    'Dataset': dataset_name,
                    'Records': basic['total_records'],
                    'Time Span (years)': basic['time_span_years'],
                    'Locations': basic['unique_locations'],
                    'Avg Content Length (chars)': round(total_avg_length, 0)
                })

        # æ‰“å°å¯¹æ¯”è¡¨æ ¼
        if metric_table:
            headers = list(metric_table[0].keys())
            col_widths = [max(len(str(row[h])) for row in metric_table) for h in headers]

            # æ‰“å°è¡¨å¤´
            header_line = '  '.join(h.ljust(w) for h, w in zip(headers, col_widths))
            print(header_line)
            print('  '.join('-' * w for w in col_widths))

            # æ‰“å°å„è¡Œ
            for row in metric_table:
                row_line = '  '.join(str(row[h]).ljust(w) for h, w in zip(headers, col_widths))
                print(row_line)

    print("="*70)

    return comparison_results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='KnowMeBenché€šç”¨æ•°æ®åˆ†æå·¥å…·')
    parser.add_argument('--dataset', type=str, default='all',
                       choices=['dataset1', 'dataset2', 'dataset3', 'all'],
                       help='è¦åˆ†æçš„æ•°æ®é›† (é»˜è®¤: all)')
    parser.add_argument('--input-dir', type=str,
                       default='./KnowmeBench',
                       help='è¾“å…¥ç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', type=str,
                       default='./analysis_output',
                       help='è¾“å‡ºç›®å½•è·¯å¾„')

    args = parser.parse_args()

    base_input_path = Path(args.input_dir)

    print("="*70)
    print("KnowMeBench é€šç”¨æ•°æ®åˆ†æå·¥å…·")
    print("="*70)

    if args.dataset == 'all':
        # æ¯”è¾ƒæ‰€æœ‰æ•°æ®é›†
        print("å¤„ç†æ‰€æœ‰æ•°æ®é›†å¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        comparison_results = compare_datasets(args.input_dir, args.output_dir)

        # ä¿å­˜å¯¹æ¯”ç»“æœ
        if args.output_dir:
            output_path = Path(args.output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            comparison_file = output_path / "dataset_comparison.json"
            with open(comparison_file, 'w', encoding='utf-8') as f:
                json.dump(comparison_results, f, indent=2, ensure_ascii=False, default=str)
            print(f"\nå¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {comparison_file}")
    else:
        # å¤„ç†å•ä¸ªæ•°æ®é›†
        input_file = base_input_path / args.dataset / 'input' / f'{args.dataset}.json'

        if not input_file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return

        process_dataset(str(input_file), 'auto', args.output_dir)

    print("\nâœ… åˆ†æå®Œæˆ!")


if __name__ == "__main__":
    main()
