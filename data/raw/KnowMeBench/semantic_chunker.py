#!/usr/bin/env python3
"""
KnowMeBenché€šç”¨è¯­ä¹‰åˆ†å—ç³»ç»Ÿ
æ”¯æŒdataset1, dataset2, dataset3çš„è‡ªé€‚åº”å¤„ç†
"""

import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
import statistics
import argparse
from pathlib import Path


class TokenEstimator:
    """Tokenä¼°ç®—å™¨"""

    @staticmethod
    def estimate_tokens(text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
        è€ƒè™‘å¤šç§å­—ç¬¦ç±»å‹ï¼š
        - è‹±æ–‡å­—ç¬¦ï¼š4å­—ç¬¦/token
        - ä¸­æ–‡å­—ç¬¦ï¼š2å­—ç¬¦/token
        - æ•°å­—ç¬¦å·ï¼šå•ç‹¬è®¡ç®—
        - ä¿å®ˆä¼°è®¡ï¼š+10%ç¼“å†²
        """
        if not text:
            return 0

        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text.strip())

        # è®¡ç®—ä¸åŒç±»å‹å­—ç¬¦
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        numbers = len(re.findall(r'\d', text))
        symbols = len(re.findall(r'[^a-zA-Z0-9\s]', text))

        # è®¡ç®—åŸºæœ¬tokenæ•°
        tokens = (english_chars / 4.0 +
                 chinese_chars / 2.0 +
                 numbers / 3.0 +
                 symbols / 2.0 +
                 text.count(' ') / 1.5)

        # æ·»åŠ 10%ç¼“å†²
        tokens = int(tokens * 1.1)

        return max(1, tokens)


class UniversalSemanticChunker:
    """é€šç”¨è¯­ä¹‰åˆ†å—å™¨ - æ”¯æŒæ‰€æœ‰ä¸‰ä¸ªdataset"""

    # å®šä¹‰ä¸‰ä¸ªdatasetçš„å­—æ®µæ˜ å°„
    FIELD_MAPPINGS = {
        'dataset1': {
            'content_fields': ['action', 'dialogue', 'environment', 'background', 'inner_thought'],
            'timestamp': 'timestamp',
            'location': 'location'
        },
        'dataset2': {
            'content_fields': ['action', 'dialogue', 'environment', 'background', 'mind'],
            'timestamp': 'timestamp',
            'location': 'location'
        },
        'dataset3': {
            'content_fields': ['action', 'dialogue', 'Environment', 'Background', 'Mind'],
            'timestamp': 'timestamp',
            'location': 'location'
        }
    }

    def __init__(self,
                 min_tokens: int = 3000,
                 max_tokens: int = 6000,
                 overlap_tokens: int = 200,
                 boundary_threshold: float = 0.5,
                 dataset_type: str = 'dataset1'):
        """
        åˆå§‹åŒ–é€šç”¨è¯­ä¹‰åˆ†å—å™¨

        Args:
            min_tokens: æœ€å°åˆ†å—å¤§å°ï¼ˆtokensï¼‰
            max_tokens: æœ€å¤§åˆ†å—å¤§å°ï¼ˆtokensï¼‰
            overlap_tokens: é‡å ä¿æŠ¤å¤§å°ï¼ˆtokensï¼‰
            boundary_threshold: è¯­ä¹‰è¾¹ç•Œå¼ºåº¦é˜ˆå€¼
            dataset_type: æ•°æ®é›†ç±»å‹ ('dataset1', 'dataset2', 'dataset3')
        """
        self.min_tokens = min_tokens
        self.max_tokens = max_tokens
        self.overlap_tokens = overlap_tokens
        self.boundary_threshold = boundary_threshold
        self.token_estimator = TokenEstimator()

        # è®¾ç½®æ•°æ®é›†ç±»å‹
        if dataset_type not in self.FIELD_MAPPINGS:
            raise ValueError(f"ä¸æ”¯æŒçš„datasetç±»å‹: {dataset_type}. æ”¯æŒçš„ç±»å‹: {list(self.FIELD_MAPPINGS.keys())}")

        self.dataset_type = dataset_type
        self.field_config = self.FIELD_MAPPINGS[dataset_type]

        print(f"åˆå§‹åŒ–åˆ†å—å™¨ - æ•°æ®é›†ç±»å‹: {dataset_type}")
        print(f"å†…å®¹å­—æ®µ: {self.field_config['content_fields']}")

    def combine_record_to_text(self, record: Dict[str, Any]) -> str:
        """
        å°†å•æ¡è®°å½•ç»„åˆæˆæ–‡æœ¬ï¼ˆè‡ªé€‚åº”ä¸åŒdatasetçš„å­—æ®µï¼‰

        Args:
            record: æ•°æ®è®°å½•

        Returns:
            ç»„åˆåçš„æ–‡æœ¬
        """
        parts = []

        # æ·»åŠ æ—¶é—´å’Œåœ°ç‚¹
        timestamp_field = self.field_config['timestamp']
        location_field = self.field_config['location']

        timestamp = record.get(timestamp_field, '')
        location = record.get(location_field, '')
        if timestamp and location:
            parts.append(f"[{timestamp}] {location}")

        # æŒ‰é…ç½®çš„å†…å®¹å­—æ®µä¼˜å…ˆçº§æ·»åŠ å†…å®¹
        content_parts = []
        for field in self.field_config['content_fields']:
            content = record.get(field)
            if content and isinstance(content, str) and content.strip():
                content_parts.append(content)

        if content_parts:
            parts.append(' '.join(content_parts))

        return ' | '.join(parts) if parts else ''

    def calculate_time_gap_strength(self, time1: str, time2: str) -> float:
        """
        è®¡ç®—æ—¶é—´è·³è·ƒå¼ºåº¦

        Args:
            time1: ç¬¬ä¸€ä¸ªæ—¶é—´æˆ³
            time2: ç¬¬äºŒä¸ªæ—¶é—´æˆ³

        Returns:
            å¼ºåº¦å€¼ (0-1)
        """
        try:
            dt1 = datetime.strptime(time1, '%Y-%m-%d %H:%M:%S')
            dt2 = datetime.strptime(time2, '%Y-%m-%d %H:%M:%S')
            gap = abs((dt2 - dt1).total_seconds())
        except (ValueError, TypeError):
            return 0.0

        # æ ¹æ®æ—¶é—´è·³è·ƒå¤§å°è®¡ç®—å¼ºåº¦
        if gap > 7 * 24 * 3600:  # è¶…è¿‡ä¸€å‘¨
            return 0.6
        elif gap > 24 * 3600:  # è¶…è¿‡ä¸€å¤©
            return 0.4
        elif gap > 6 * 3600:  # è¶…è¿‡6å°æ—¶
            return 0.2
        else:
            return 0.1

    def calculate_location_change_strength(self, loc1: str, loc2: str) -> float:
        """
        è®¡ç®—åœ°ç‚¹å˜åŒ–å¼ºåº¦

        Args:
            loc1: ç¬¬ä¸€ä¸ªåœ°ç‚¹
            loc2: ç¬¬äºŒä¸ªåœ°ç‚¹

        Returns:
            å¼ºåº¦å€¼ (0-1)
        """
        if not loc1 or not loc2:
            return 0.0

        if loc1 != loc2:
            return 0.3

        return 0.0

    def calculate_content_density_change(self,
                                       prev_content_length: int,
                                       curr_content_length: int) -> float:
        """
        è®¡ç®—å†…å®¹å¯†åº¦å˜åŒ–å¼ºåº¦

        Args:
            prev_content_length: å‰ä¸€æ¡è®°å½•å†…å®¹é•¿åº¦
            curr_content_length: å½“å‰è®°å½•å†…å®¹é•¿åº¦

        Returns:
            å¼ºåº¦å€¼ (0-1)
        """
        if prev_content_length == 0:
            return 0.0

        density_ratio = abs(curr_content_length - prev_content_length) / prev_content_length

        if density_ratio > 0.5:
            return 0.2
        elif density_ratio > 0.3:
            return 0.15
        else:
            return 0.05

    def calculate_content_anomaly_strength(self, content_length: int,
                                         avg_content_length: float) -> float:
        """
        è®¡ç®—å†…å®¹é•¿åº¦å¼‚å¸¸å¼ºåº¦

        Args:
            content_length: å½“å‰å†…å®¹é•¿åº¦
            avg_content_length: å¹³å‡å†…å®¹é•¿åº¦

        Returns:
            å¼ºåº¦å€¼ (0-1)
        """
        if avg_content_length == 0:
            return 0.0

        ratio = content_length / avg_content_length

        if ratio > 3.0 or ratio < 0.3:
            return 0.15
        else:
            return 0.0

    def detect_semantic_boundary(self,
                               prev_record: Dict[str, Any],
                               curr_record: Dict[str, Any],
                               avg_content_length: float) -> Tuple[bool, float]:
        """
        æ£€æµ‹è¯­ä¹‰è¾¹ç•Œ

        Args:
            prev_record: å‰ä¸€æ¡è®°å½•
            curr_record: å½“å‰è®°å½•
            avg_content_length: å¹³å‡å†…å®¹é•¿åº¦

        Returns:
            (æ˜¯å¦è¯­ä¹‰è¾¹ç•Œ, è¾¹ç•Œå¼ºåº¦)
        """
        # è®¡ç®—å„ç§è¾¹ç•ŒæŒ‡æ ‡
        time_strength = 0.0
        location_strength = 0.0
        density_strength = 0.0
        anomaly_strength = 0.0

        timestamp_field = self.field_config['timestamp']
        location_field = self.field_config['location']

        # æ—¶é—´è·³è·ƒæ£€æµ‹
        if prev_record.get(timestamp_field) and curr_record.get(timestamp_field):
            time_strength = self.calculate_time_gap_strength(
                prev_record[timestamp_field],
                curr_record[timestamp_field]
            )

        # åœ°ç‚¹å˜åŒ–æ£€æµ‹
        location_strength = self.calculate_location_change_strength(
            prev_record.get(location_field, ''),
            curr_record.get(location_field, '')
        )

        # å†…å®¹å¯†åº¦å˜åŒ–æ£€æµ‹
        prev_content = self._get_record_content_length(prev_record)
        curr_content = self._get_record_content_length(curr_record)
        density_strength = self.calculate_content_density_change(
            prev_content, curr_content
        )

        # å†…å®¹é•¿åº¦å¼‚å¸¸æ£€æµ‹
        anomaly_strength = self.calculate_content_anomaly_strength(
            curr_content, avg_content_length
        )

        # ç»¼åˆè¾¹ç•Œå¼ºåº¦
        boundary_strength = (time_strength + location_strength +
                           density_strength + anomaly_strength)

        # åˆ¤æ–­æ˜¯å¦ä¸ºè¯­ä¹‰è¾¹ç•Œ
        is_semantic_boundary = boundary_strength >= self.boundary_threshold

        return is_semantic_boundary, boundary_strength

    def _get_record_content_length(self, record: Dict[str, Any]) -> int:
        """è·å–è®°å½•å†…å®¹é•¿åº¦ï¼ˆè‡ªé€‚åº”å­—æ®µï¼‰"""
        total_length = 0

        for field in self.field_config['content_fields']:
            content = record.get(field)
            if content and isinstance(content, str):
                total_length += len(content)

        return total_length

    def create_chunks(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        åˆ›å»ºè¯­ä¹‰åˆ†å—

        Args:
            data: æ•°æ®è®°å½•åˆ—è¡¨

        Returns:
            åˆ†å—åˆ—è¡¨
        """
        print("å¼€å§‹è¯­ä¹‰åˆ†å—...")

        # è®¡ç®—å¹³å‡å†…å®¹é•¿åº¦
        content_lengths = [self._get_record_content_length(record) for record in data]
        avg_content_length = statistics.mean(content_lengths) if content_lengths else 0

        chunks = []
        current_chunk = {
            'chunk_id': 0,
            'text': '',
            'start_id': 0,
            'end_id': 0,
            'record_count': 0,
            'token_count': 0,
            'start_time': None,
            'end_time': None,
            'locations': []
        }

        timestamp_field = self.field_config['timestamp']
        location_field = self.field_config['location']

        for i, record in enumerate(data):
            # ç»„åˆå½“å‰è®°å½•ä¸ºæ–‡æœ¬
            record_text = self.combine_record_to_text(record)

            if not record_text:
                continue

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²
            if current_chunk['text']:
                # è®¡ç®—æ–°chunkå¤§å°
                new_token_count = self.token_estimator.estimate_tokens(
                    current_chunk['text'] + '\n' + record_text
                )

                # å¦‚æœè¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œéœ€è¦åˆ†å‰²
                if new_token_count > self.max_tokens:
                    # ä¿å­˜å½“å‰chunk
                    current_chunk['end_id'] = i - 1
                    current_chunk['token_count'] = self.token_estimator.estimate_tokens(
                        current_chunk['text']
                    )
                    chunks.append(current_chunk.copy())

                    # åˆ›å»ºæ–°chunk
                    current_chunk = {
                        'chunk_id': len(chunks),
                        'text': '',
                        'start_id': i,
                        'end_id': 0,
                        'record_count': 0,
                        'token_count': 0,
                        'start_time': record.get(timestamp_field),
                        'end_time': None,
                        'locations': []
                    }

                # æ£€æŸ¥è¯­ä¹‰è¾¹ç•Œï¼ˆä»…å½“å½“å‰chunkè¶…è¿‡æœ€å°å¤§å°æ—¶ï¼‰
                elif new_token_count >= self.min_tokens:
                    is_boundary, strength = self.detect_semantic_boundary(
                        data[i-1], record, avg_content_length
                    )

                    if is_boundary:
                        # ä¿å­˜å½“å‰chunk
                        current_chunk['end_id'] = i - 1
                        current_chunk['token_count'] = self.token_estimator.estimate_tokens(
                            current_chunk['text']
                        )
                        chunks.append(current_chunk.copy())

                        # åˆ›å»ºæ–°chunk
                        current_chunk = {
                            'chunk_id': len(chunks),
                            'text': '',
                            'start_id': i,
                            'end_id': 0,
                            'record_count': 0,
                            'token_count': 0,
                            'start_time': record.get(timestamp_field),
                            'end_time': None,
                            'locations': []
                        }

            # æ·»åŠ è®°å½•åˆ°å½“å‰chunk
            if current_chunk['text']:
                current_chunk['text'] += '\n' + record_text
            else:
                current_chunk['text'] = record_text
                current_chunk['start_time'] = record.get(timestamp_field)

            current_chunk['end_time'] = record.get(timestamp_field)
            current_chunk['record_count'] += 1

            # è®°å½•åœ°ç‚¹
            location = record.get(location_field)
            if location and location not in current_chunk['locations']:
                current_chunk['locations'].append(location)

        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk['text']:
            current_chunk['end_id'] = len(data) - 1
            current_chunk['token_count'] = self.token_estimator.estimate_tokens(
                current_chunk['text']
            )
            chunks.append(current_chunk)

        print(f"å®Œæˆè¯­ä¹‰åˆ†å—ï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªchunk")

        return chunks

    def save_chunks(self,
                   chunks: List[Dict[str, Any]],
                   output_dir: str,
                   dataset_name: str) -> None:
        """
        ä¿å­˜åˆ†å—ç»“æœ

        Args:
            chunks: åˆ†å—åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            dataset_name: æ•°æ®é›†åç§°
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # æ–‡ä»¶è·¯å¾„
        output_json = output_path / f"{dataset_name}_chunks.json"
        output_text = output_path / f"{dataset_name}_chunks_text.txt"

        # ä¿å­˜JSONæ ¼å¼ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(chunks, f, indent=2, ensure_ascii=False)

        print(f"JSONæ ¼å¼åˆ†å—å·²ä¿å­˜åˆ°: {output_json}")

        # ä¿å­˜çº¯æ–‡æœ¬æ ¼å¼ï¼ˆä¾¿äºé˜…è¯»ï¼‰
        with open(output_text, 'w', encoding='utf-8') as f:
            for i, chunk in enumerate(chunks):
                f.write(f"\n{'='*80}\n")
                f.write(f"Chunk {i+1}/{len(chunks)}\n")
                f.write(f"{'='*80}\n")
                f.write(f"ID: {chunk['chunk_id']}\n")
                f.write(f"è®°å½•èŒƒå›´: {chunk['start_id']} - {chunk['end_id']}\n")
                f.write(f"è®°å½•æ•°: {chunk['record_count']}\n")
                f.write(f"Tokenæ•°: {chunk['token_count']}\n")
                f.write(f"æ—¶é—´èŒƒå›´: {chunk['start_time']} - {chunk['end_time']}\n")
                f.write(f"åœ°ç‚¹æ•°: {len(chunk['locations'])}\n")
                f.write(f"\n{chunk['text']}\n")

        print(f"çº¯æ–‡æœ¬æ ¼å¼åˆ†å—å·²ä¿å­˜åˆ°: {output_text}")

    def print_chunk_statistics(self, chunks: List[Dict[str, Any]]) -> None:
        """
        æ‰“å°åˆ†å—ç»Ÿè®¡ä¿¡æ¯

        Args:
            chunks: åˆ†å—åˆ—è¡¨
        """
        if not chunks:
            print("æ²¡æœ‰åˆ†å—æ•°æ®")
            return

        token_counts = [chunk['token_count'] for chunk in chunks]

        # ç»Ÿè®¡ä¸åŒå¤§å°çš„chunkæ•°é‡
        size_distribution = {
            '<3k': 0,
            '3k-4k': 0,
            '4k-5k': 0,
            '5k-6k': 0,
            '>6k': 0
        }

        for token_count in token_counts:
            if token_count < 3000:
                size_distribution['<3k'] += 1
            elif token_count < 4000:
                size_distribution['3k-4k'] += 1
            elif token_count < 5000:
                size_distribution['4k-5k'] += 1
            elif token_count < 6000:
                size_distribution['5k-6k'] += 1
            else:
                size_distribution['>6k'] += 1

        print("\n" + "="*60)
        print(f"è¯­ä¹‰åˆ†å—ç»Ÿè®¡ç»“æœ - {self.dataset_type}")
        print("="*60)

        print(f"\nğŸ“Š åˆ†å—æ¦‚è§ˆ:")
        print(f"  æ€»åˆ†å—æ•°: {len(chunks)}")
        print(f"  å¹³å‡å¤§å°: {round(statistics.mean(token_counts))} tokens")
        print(f"  ä¸­ä½æ•°å¤§å°: {round(statistics.median(token_counts))} tokens")
        print(f"  æœ€å°å¤§å°: {min(token_counts)} tokens")
        print(f"  æœ€å¤§å¤§å°: {max(token_counts)} tokens")

        print(f"\nğŸ“ˆ å¤§å°åˆ†å¸ƒ:")
        for size_range, count in size_distribution.items():
            percentage = (count / len(chunks)) * 100
            print(f"  {size_range}: {count} ä¸ª ({percentage:.1f}%)")

        print(f"\nğŸ“ åœ°ç‚¹è¦†ç›–:")
        all_locations = set()
        for chunk in chunks:
            all_locations.update(chunk['locations'])
        print(f"  æ€»æ¶‰åŠåœ°ç‚¹: {len(all_locations)} ä¸ª")

        print("\n" + "="*60)


def process_dataset(input_file: str, output_dir: str, dataset_type: str,
                   min_tokens: int = 3000, max_tokens: int = 6000):
    """
    å¤„ç†å•ä¸ªæ•°æ®é›†

    Args:
        input_file: è¾“å…¥JSONæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        dataset_type: æ•°æ®é›†ç±»å‹
        min_tokens: æœ€å°tokenæ•°
        max_tokens: æœ€å¤§tokenæ•°
    """
    # åˆ›å»ºåˆ†å—å™¨
    chunker = UniversalSemanticChunker(
        min_tokens=min_tokens,
        max_tokens=max_tokens,
        overlap_tokens=200,
        boundary_threshold=0.5,
        dataset_type=dataset_type
    )

    try:
        # åŠ è½½æ•°æ®
        print(f"\nåŠ è½½æ•°æ®é›†: {input_file}")
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•")

        # æ‰§è¡Œåˆ†å—
        chunks = chunker.create_chunks(data)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        chunker.print_chunk_statistics(chunks)

        # ä¿å­˜ç»“æœ
        chunker.save_chunks(chunks, output_dir, dataset_type)

        print(f"\nâœ… {dataset_type} è¯­ä¹‰åˆ†å—å®Œæˆ!")

        return chunks

    except Exception as e:
        print(f"âŒ å¤„ç† {dataset_type} æ—¶å‡ºç°é”™è¯¯: {e}")
        raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='KnowMeBenché€šç”¨è¯­ä¹‰åˆ†å—å·¥å…·')
    parser.add_argument('--dataset', type=str, default='all',
                       choices=['dataset1', 'dataset2', 'dataset3', 'all'],
                       help='è¦å¤„ç†çš„æ•°æ®é›† (é»˜è®¤: all)')
    parser.add_argument('--input-dir', type=str,
                       default='./KnowmeBench',
                       help='è¾“å…¥ç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', type=str,
                       default='./chunked_output',
                       help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--min-tokens', type=int, default=3000,
                       help='æœ€å°chunkå¤§å°ï¼ˆtokensï¼‰')
    parser.add_argument('--max-tokens', type=int, default=6000,
                       help='æœ€å¤§chunkå¤§å°ï¼ˆtokensï¼‰')

    args = parser.parse_args()

    base_input_path = Path(args.input_dir)

    # å®šä¹‰æ•°æ®é›†é…ç½®
    dataset_configs = {
        'dataset1': base_input_path / 'dataset1/input/dataset1.json',
        'dataset2': base_input_path / 'dataset2/input/dataset2.json',
        'dataset3': base_input_path / 'dataset3/input/dataset3.json'
    }

    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†
    if args.dataset == 'all':
        datasets_to_process = list(dataset_configs.keys())
    else:
        datasets_to_process = [args.dataset]

    print("="*60)
    print("KnowMeBench é€šç”¨è¯­ä¹‰åˆ†å—å·¥å…·")
    print("="*60)
    print(f"å¤„ç†æ•°æ®é›†: {', '.join(datasets_to_process)}")
    print(f"TokenèŒƒå›´: {args.min_tokens} - {args.max_tokens}")
    print("="*60)

    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    results = {}
    for dataset_name in datasets_to_process:
        input_file = dataset_configs[dataset_name]

        if not input_file.exists():
            print(f"\nâš ï¸  è·³è¿‡ {dataset_name}: æ–‡ä»¶ä¸å­˜åœ¨ - {input_file}")
            continue

        try:
            chunks = process_dataset(
                str(input_file),
                args.output_dir,
                dataset_name,
                args.min_tokens,
                args.max_tokens
            )
            results[dataset_name] = chunks
        except Exception as e:
            print(f"\nâŒ {dataset_name} å¤„ç†å¤±è´¥: {e}")
            continue

    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("å¤„ç†æ€»ç»“")
    print("="*60)
    for dataset_name, chunks in results.items():
        print(f"{dataset_name}: {len(chunks)} ä¸ªchunks")
    print("="*60)
    print("\nâœ… å…¨éƒ¨å¤„ç†å®Œæˆ!")


if __name__ == "__main__":
    main()
