{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ed2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_dev.parquet\"\n",
    "df = pd.read_parquet(file_path)\n",
    "# df['agent_name'] = 'mem_agent'\n",
    "# df.to_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_dev_agent_loop.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2bdbcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®æ¡æ•°ï¼š500\n",
      "æ­£ç¡®ç‡ï¼š0.382\n",
      "knowledge-update 0.4358974358974359\n",
      "34 / 78\n",
      "single-session-user 0.4142857142857143\n",
      "29 / 70\n",
      "temporal-reasoning 0.46616541353383456\n",
      "62 / 133\n",
      "single-session-preference 0.6\n",
      "18 / 30\n",
      "single-session-assistant 0.17857142857142858\n",
      "10 / 56\n",
      "multi-session 0.2857142857142857\n",
      "38 / 133\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/HippoRAG/longmemeval_s_r1_log_no_longcontext_recall20_eval_res.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/longmemeval_s/Qwen3-8B-5k-1k-infty_eval_res.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/processing_log_qwen3-8b_1758104883.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/processing_log_qwen3-8b_faster_1758253073.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/results/qwen3-8b/longmemeval/evaluated_concat.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/results/qwen3-8b/longmemeval_oracle/evaluated_concat.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/ReMe/cookbook/simple_demo/longmemeval/evaluated_unknown.jsonl\"\n",
    "data = [json.loads(line) for line in open(datapath)]\n",
    "\n",
    "original_datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/data/longmemeval_s.json\"\n",
    "original_data = json.load(open(original_datapath))\n",
    "for d in data:\n",
    "    for od in original_data:\n",
    "        question_id = d.get(\"question_id\", d.get(\"qid\"))\n",
    "        if \"longmemeval_\" in question_id:\n",
    "            question_id = question_id[len(\"longmemeval_\"):-2]\n",
    "        if od[\"question_id\"] == question_id:\n",
    "            d[\"question_type\"] = od[\"question_type\"]\n",
    "            break\n",
    "\n",
    "# datapath = \"longmemeval_s_r1_log_no_longcontext_recall20.json\"\n",
    "# data = json.load(open(datapath))\n",
    "print(\"æ•°æ®æ¡æ•°ï¼š\" + str(len([x for x in data if x.get('qid') is not None])))\n",
    "print(\"æ­£ç¡®ç‡ï¼š\" + str(len([x for x in data if x[\"metric\"].get('llm_score')]) / len([x for x in data if x[\"metric\"].get('llm_score') is not None])))\n",
    "\n",
    "for tp in [\"knowledge-update\", \"single-session-user\", \"temporal-reasoning\", \"single-session-preference\", \"single-session-assistant\", \"multi-session\"]:\n",
    "    if len([x for x in data if x.get('question_type')==tp]) == 0:\n",
    "        print(tp, 0)\n",
    "    else:\n",
    "        print(tp, len([x for x in data if x[\"metric\"].get('llm_score') and x.get('question_type')==tp]) / len([x for x in data if x.get('question_type')==tp]))\n",
    "        print(f\"{len([x for x in data if x[\"metric\"].get('llm_score') and x.get('question_type')==tp])} / {len([x for x in data if x.get('question_type')==tp])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74531ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®æ¡æ•°ï¼š1986\n",
      "æ­£ç¡®ç‡ï¼š0.28751258811681774\n",
      "multi hop 0.09574468085106383\n",
      "27 / 282\n",
      "temporal 0.07476635514018691\n",
      "24 / 321\n",
      "open domain 0.21875\n",
      "21 / 96\n",
      "single hop 0.06539833531510107\n",
      "55 / 841\n",
      "adversarial 0.9955156950672646\n",
      "444 / 446\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/results/qwen3-8b/locomo/evaluated_concat.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/locomo_results_qwen3-8b_faster_1759028934.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/locomo_results_1759036272.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/locomo/Qwen3-8B-5k-1k-infty.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/HippoRAG/locomo_hippo_qwen_recall42.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/HippoRAG/locomo_hippo_qwen_recall20.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/ReMe/cookbook/simple_demo/locomo/evaluated_unknown.jsonl\"\n",
    "data = [json.loads(line) for line in open(datapath)]\n",
    "# data = sum([d[\"queries_solutions\"] for d in data], start=[])\n",
    "\n",
    "original_datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/raw/locomo10.json\"\n",
    "original_data = json.load(open(original_datapath))\n",
    "\n",
    "for d in data:\n",
    "    for od in original_data:\n",
    "        if od['sample_id'] in d['qid']:\n",
    "            q_index = int(d['qid'].split('_')[2])\n",
    "            d['question_type'] = od['qa'][q_index]['category']\n",
    "\n",
    "for d in data:\n",
    "    # d['question_type'] = d['category']\n",
    "    d[\"gpt4o_score\"] =  d[\"metric\"].get('llm_score')\n",
    "\n",
    "print(\"æ•°æ®æ¡æ•°ï¼š\" + str(len(data)))\n",
    "print(\"æ­£ç¡®ç‡ï¼š\" + str(len([x for x in data if x[\"gpt4o_score\"]]) / len([x for x in data if x[\"gpt4o_score\"] is not None])))\n",
    "\n",
    "question_type_map = {\n",
    "    1: \"multi hop\",\n",
    "    2: \"temporal\",\n",
    "    3: \"open domain\",\n",
    "    4: \"single hop\",\n",
    "    5: \"adversarial\"\n",
    "}\n",
    "\n",
    "\n",
    "for tp in range(1,6):\n",
    "    if len([x for x in data if x.get('question_type')==tp]) == 0:\n",
    "        print(question_type_map[tp], 0)\n",
    "    else:\n",
    "        print(question_type_map[tp], len([x for x in data if x[\"gpt4o_score\"] and x.get('question_type')==tp]) / len([x for x in data if x.get('question_type')==tp]))\n",
    "        print(f\"{len([x for x in data if x[\"gpt4o_score\"] and x.get('question_type')==tp])} / {len([x for x in data if x.get('question_type')==tp])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "from openai import OpenAI, RateLimitError\n",
    "import uuid\n",
    "import backoff\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm as atqdm\n",
    "\n",
    "from config import MODEL_NAME, API_CONFIG_LOCAL\n",
    "from openai import OpenAI\n",
    "\n",
    "async def gpt4o_evaluation(question, correct_answer, model_response, question_type):\n",
    "    \"\"\"\n",
    "    Use GPT-4o to evaluate the model response\n",
    "    Based on reprocess_res.py\n",
    "    \"\"\"\n",
    "\n",
    "    API_CONFIG_LOCAL[\"base_url\"] = \"http://172.24.139.15:8000/v1\"\n",
    "    openai_client = OpenAI(**API_CONFIG_LOCAL)\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        RateLimitError,\n",
    "        max_tries=16,\n",
    "        max_time=300,\n",
    "        jitter=backoff.full_jitter\n",
    "    )\n",
    "    async def llm_response(messages):\n",
    "        # ä½¿ç”¨ asyncio.to_thread å°†åŒæ­¥è°ƒç”¨è½¬ä¸ºå¼‚æ­¥\n",
    "        res = await asyncio.to_thread(\n",
    "            openai_client.chat.completions.create,\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "        response_message = res.choices[0].message.content\n",
    "        if \"</think>\" in response_message:\n",
    "            response_message = response_message.split(\"</think>\")[-1].strip()\n",
    "        # print(response_message)\n",
    "        return response_message\n",
    "    \n",
    "    def get_anscheck_prompt(task, question, answer, response):\n",
    "        LLM_TEMPLATE = \"I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no.\\n\\nQuestion: {}\\n\\nCorrect Answer: {}\\n\\nModel Response: {}\\n\\nIs the model response correct? Answer yes or no only.\"\n",
    "        prompt = LLM_TEMPLATE.format(question, answer, response)\n",
    "        return prompt\n",
    "    \n",
    "    async def anscheck(task, question, answer, response):\n",
    "        prompt = get_anscheck_prompt(task, question, answer, response)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        res = await llm_response(messages)\n",
    "        return 'yes' in res.lower() and not 'no' in res.lower()\n",
    "    \n",
    "    return await anscheck(question_type, question, correct_answer, model_response)\n",
    "\n",
    "async def process_item(d, output_file, semaphore, file_lock):\n",
    "    async with semaphore:\n",
    "        score = await gpt4o_evaluation(d[\"input\"], d[\"answer\"], d[\"response\"], d[\"question_type\"])\n",
    "        d[\"gpt4o_score\"] = score\n",
    "        \n",
    "        # ä½¿ç”¨æ–‡ä»¶é”ç¡®ä¿å¹¶å‘å†™å…¥å®‰å…¨\n",
    "        async with file_lock:\n",
    "            with open(output_file, \"a\") as f:\n",
    "                f.write(json.dumps(d) + '\\n')\n",
    "        \n",
    "        return d\n",
    "\n",
    "async def main():\n",
    "    data_path = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/locomo/Qwen3-8B-5k-1k-infty.jsonl\"\n",
    "    data = [json.loads(line) for line in open(data_path)]\n",
    "    \n",
    "    output_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/locomo/Qwen3-8B-5k-1k-infty_.jsonl\"\n",
    "    \n",
    "    # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°\n",
    "    semaphore = asyncio.Semaphore(20)  # å¯è°ƒæ•´å¹¶å‘æ•°\n",
    "    \n",
    "    # åˆ›å»ºæ–‡ä»¶é”ï¼Œç¡®ä¿å¹¶å‘å†™å…¥å®‰å…¨\n",
    "    file_lock = asyncio.Lock()\n",
    "    \n",
    "    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡\n",
    "    tasks = [process_item(d, output_file, semaphore, file_lock) for d in data]\n",
    "    \n",
    "    # ä½¿ç”¨å¼‚æ­¥è¿›åº¦æ¡æ‰§è¡Œ\n",
    "    results = await atqdm.gather(*tasks, desc=\"Processing\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # asyncio.run(main())\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348906d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EvalDataset] Registered benchmarks: banking77, booksum, clinic, convomem, hotpotqa, infbench, locomo, longmemeval, memalpha, msc, nlu, perltqa, pubmed_rct, squad, synth, trec_coarse, trec_fine\n",
      "Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='Hello', bytes=[72, 101, 108, 108, 111], logprob=-0.04368528351187706, top_logprobs=[]), ChatCompletionTokenLogprob(token='!', bytes=[33], logprob=-1.0728830375228426e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=' How', bytes=[32, 72, 111, 119], logprob=-0.3034718334674835, top_logprobs=[]), ChatCompletionTokenLogprob(token=' can', bytes=[32, 99, 97, 110], logprob=-9.536697689327411e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=' I', bytes=[32, 73], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token=' assist', bytes=[32, 97, 115, 115, 105, 115, 116], logprob=-0.07889139652252197, top_logprobs=[]), ChatCompletionTokenLogprob(token=' you', bytes=[32, 121, 111, 117], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token=' today', bytes=[32, 116, 111, 100, 97, 121], logprob=-9.536738616588991e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='?', bytes=[63], logprob=-3.576278118089249e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token=' ï¿½', bytes=[32, 239, 191, 189], logprob=-0.0004667146422434598, top_logprobs=[]), ChatCompletionTokenLogprob(token='ï¿½', bytes=[239, 191, 189], logprob=-3.1470757676288486e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token='<|im_end|>', bytes=[60, 124, 105, 109, 95, 101, 110, 100, 124, 62], logprob=-3.9219088648678735e-05, top_logprobs=[])], refusal=None), message=ChatCompletionMessage(content='Hello! How can I assist you today? ğŸ˜Š', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None, token_ids=None)\n"
     ]
    }
   ],
   "source": [
    "from config import MODEL_NAME, API_CONFIG_LOCAL, API_CONFIG\n",
    "from openai import OpenAI\n",
    "API_CONFIG_LOCAL[\"base_url\"] = \"http://172.24.139.15:8000/v1\"\n",
    "openai_client = OpenAI(**API_CONFIG_LOCAL)\n",
    "# openai_client = OpenAI(**API_CONFIG)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hello\"\n",
    "    }\n",
    "]\n",
    "res = openai_client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    logprobs=1\n",
    ")\n",
    "print(res.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610405ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='Hello', bytes=[72, 101, 108, 108, 111], logprob=-0.04368528351187706, top_logprobs=[]), ChatCompletionTokenLogprob(token='!', bytes=[33], logprob=-1.0728830375228426e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=' How', bytes=[32, 72, 111, 119], logprob=-0.3034718334674835, top_logprobs=[]), ChatCompletionTokenLogprob(token=' can', bytes=[32, 99, 97, 110], logprob=-9.536697689327411e-06, top_logprobs=[]), ChatCompletionTokenLogprob(token=' I', bytes=[32, 73], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token=' assist', bytes=[32, 97, 115, 115, 105, 115, 116], logprob=-0.07889139652252197, top_logprobs=[]), ChatCompletionTokenLogprob(token=' you', bytes=[32, 121, 111, 117], logprob=0.0, top_logprobs=[]), ChatCompletionTokenLogprob(token=' today', bytes=[32, 116, 111, 100, 97, 121], logprob=-9.536738616588991e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token='?', bytes=[63], logprob=-3.576278118089249e-07, top_logprobs=[]), ChatCompletionTokenLogprob(token=' ï¿½', bytes=[32, 239, 191, 189], logprob=-0.0004667146422434598, top_logprobs=[]), ChatCompletionTokenLogprob(token='ï¿½', bytes=[239, 191, 189], logprob=-3.1470757676288486e-05, top_logprobs=[]), ChatCompletionTokenLogprob(token='<|im_end|>', bytes=[60, 124, 105, 109, 95, 101, 110, 100, 124, 62], logprob=-3.9219088648678735e-05, top_logprobs=[])], refusal=None), message=ChatCompletionMessage(content='Hello! How can I assist you today? ğŸ˜Š', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None, token_ids=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc7d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<think>',\n",
       " '\\n',\n",
       " 'Okay',\n",
       " ',',\n",
       " ' the',\n",
       " ' user',\n",
       " ' said',\n",
       " ' \"',\n",
       " 'hello',\n",
       " '\".',\n",
       " ' I',\n",
       " ' need',\n",
       " ' to',\n",
       " ' respond',\n",
       " ' appropriately',\n",
       " '.',\n",
       " ' Since',\n",
       " ' it',\n",
       " \"'s\",\n",
       " ' a',\n",
       " ' greeting',\n",
       " ',',\n",
       " ' I',\n",
       " ' should',\n",
       " ' acknowledge',\n",
       " ' it',\n",
       " ' and',\n",
       " ' offer',\n",
       " ' assistance',\n",
       " '.',\n",
       " ' Let',\n",
       " ' me',\n",
       " ' make',\n",
       " ' sure',\n",
       " ' the',\n",
       " ' response',\n",
       " ' is',\n",
       " ' friendly',\n",
       " ' and',\n",
       " ' open',\n",
       " '-ended',\n",
       " '.',\n",
       " ' Maybe',\n",
       " ' ask',\n",
       " ' how',\n",
       " ' I',\n",
       " ' can',\n",
       " ' help',\n",
       " ' them',\n",
       " ' today',\n",
       " '.',\n",
       " ' Keep',\n",
       " ' it',\n",
       " ' simple',\n",
       " ' and',\n",
       " ' welcoming',\n",
       " '.\\n',\n",
       " '</think>',\n",
       " '\\n\\n',\n",
       " 'Hello',\n",
       " '!',\n",
       " ' How',\n",
       " ' can',\n",
       " ' I',\n",
       " ' assist',\n",
       " ' you',\n",
       " ' today',\n",
       " '?',\n",
       " ' ï¿½',\n",
       " 'ï¿½',\n",
       " '<|im_end|>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.token for x in res.choices[0].logprobs.content]\n",
    "res.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5574cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_context() -> str:\n",
    "    \"\"\"Load contextual information fed into ``ToolMemoryAgentLoop``.\n",
    "\n",
    "    Projects can override this helper to supply domain-specific context. By default it\n",
    "    returns the value from ``TOOL_MEM_AGENT_TEST_CONTEXT`` (or an empty string).\n",
    "    \"\"\"\n",
    "\n",
    "    # return os.environ.get(\"TOOL_MEM_AGENT_TEST_CONTEXT\", \"I like to eat bananas.\")\n",
    "    raw = json.load(open(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/zep/benchmarks/longmemeval/data/longmemeval_s.json\"))\n",
    "    item0 = raw[0]\n",
    "    context = \"Below is a conversation between User and Assistant.\\n\\n\"\n",
    "    for idx, session in enumerate(item0[\"haystack_sessions\"]):\n",
    "        session_date = item0[\"haystack_dates\"][idx]\n",
    "        session_conv = ''\n",
    "        for turn in session:\n",
    "            role = \"User\" if turn['role'] == 'user' else \"Assistant\"\n",
    "            turn_text = f'{role} said, \"{turn[\"content\"]}\"'\n",
    "            turn_text += '\\n'\n",
    "            session_conv += turn_text\n",
    "        if not session_conv:\n",
    "            session_conv = \"NO CONVERSATION\"\n",
    "        query_conv = f'DATE: {session_date}\\nCONVERSATION:\\n{session_conv}\\n\\n'\n",
    "        context += query_conv\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat\n",
    "import json\n",
    "input_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/verl/base_result.jsonl\"\n",
    "output_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/verl/base_result_reformat.jsonl\"\n",
    "original_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/zep/benchmarks/longmemeval/data/longmemeval_s.json\"\n",
    "data = [json.loads(line) for line in open(input_file)]\n",
    "original_data = json.load(open(original_file))\n",
    "with open(output_file, \"w\") as f:\n",
    "    for d in data:\n",
    "        original_d = original_data[d['idx']]\n",
    "        response = d[\"response\"]\n",
    "        if \"\\\\boxed{\" in response and \"}\" in response.split(\"\\\\boxed{\")[-1]:\n",
    "            response = response.split(\"\\\\boxed{\")[-1]\n",
    "            response = response[:response.rindex(\"}\")]\n",
    "        new_d = {\n",
    "            \"qid\": \"longmemeval_\" + original_d[\"question_id\"] + \"_0\",\n",
    "            \"query\": original_d[\"question\"],\n",
    "            \"expected_answer\": original_d[\"answer\"],\n",
    "            \"response\": response\n",
    "        }\n",
    "        f.write(json.dumps(new_d) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7162259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/synth_dev_verl_new.parquet\")\n",
    "sum([len(x[0]['content']) for x in df['prompt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d4a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memalpha\n",
      "4094 records\n",
      "9.059843673668784 chunks\n",
      "5036.917850691542 chars\n",
      "1315.996122370732 tokens\n",
      "locomo\n",
      "20 records\n",
      "19.0 chunks\n",
      "3942.8947368421054 chars\n",
      "928.1052631578945 tokens\n",
      "hotpotqa\n",
      "8192 records\n",
      "61.06103515625 chunks\n",
      "1701.0528355977065 chars\n",
      "420.35425080849217 tokens\n",
      "synth\n",
      "200 records\n",
      "10.0 chunks\n",
      "2383.626 chars\n",
      "825.3845000000001 tokens\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", use_fast=True)\n",
    "training_files = {\n",
    "    'memalpha': '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalphafull_train_verl.parquet',\n",
    "    'locomo': '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/locomo_train_verl.parquet',\n",
    "    'hotpotqa': '/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_train_mem_agent_loop_chunk2000.parquet',\n",
    "    'synth': '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/synth_train_verl_new.parquet'\n",
    "}\n",
    "for name, input_file in training_files.items():\n",
    "    print(name)\n",
    "    df = pd.read_parquet(input_file)\n",
    "    print(len(df), 'records')\n",
    "    print(np.mean([len([len(x) for x in y['tools_kwargs']['memory_bm25_retrieve']['create_kwargs']['chunks']]) for y in df['extra_info']]), 'chunks') # å¹³å‡chunksæ•°é‡\n",
    "    print(np.mean([len(x) for y in df['extra_info'] for x in y['tools_kwargs']['memory_bm25_retrieve']['create_kwargs']['chunks']]), 'chars') # å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡\n",
    "    print(np.mean([np.mean([len(tokenizer.tokenize(x)) for x in y['tools_kwargs']['memory_bm25_retrieve']['create_kwargs']['chunks']]) for y in df['extra_info']]), 'tokens') #å¹³å‡æ¯ä¸ªchunktokenæ•°é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18ef485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotpotqa_file\n",
      "æ•°æ®æ¡æ•°ï¼š128\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š1.0\n",
      "å¹³å‡chunksæ•°é‡: 200.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 570.0202734375 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 140.5790625 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 27945.2890625 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 114004.0546875 chars\n",
      "\n",
      "locomo_file\n",
      "æ•°æ®æ¡æ•°ï¼š10\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š198.6\n",
      "å¹³å‡chunksæ•°é‡: 27.2 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 3398.1507352941176 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 814.7279411764706 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 22160.6 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 92429.7 chars\n",
      "\n",
      "longmemeval_file\n",
      "æ•°æ®æ¡æ•°ï¼š500\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š1.0\n",
      "å¹³å‡chunksæ•°é‡: 47.764 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 10521.834394104346 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 2258.246838623231 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 107862.902 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 502564.898 chars\n",
      "\n",
      "msc_file\n",
      "æ•°æ®æ¡æ•°ï¼š100\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š5.0\n",
      "å¹³å‡chunksæ•°é‡: 25.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 1563.9144 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 394.1004 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 9852.51 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 39097.86 chars\n",
      "\n",
      "banking77_file\n",
      "æ•°æ®æ¡æ•°ï¼š1\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š100.0\n",
      "å¹³å‡chunksæ•°é‡: 111.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 4279.54954954955 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 1150.2522522522522 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 127568.0 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 475030.0 chars\n",
      "\n",
      "booksum_file\n",
      "æ•°æ®æ¡æ•°ï¼š155\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š1.0\n",
      "å¹³å‡chunksæ•°é‡: 8.135483870967741 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 7643.934179222839 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 1914.3243457573355 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 15566.81935483871 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 62187.10322580645 chars\n",
      "\n",
      "clinic_file\n",
      "æ•°æ®æ¡æ•°ï¼š1\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š100.0\n",
      "å¹³å‡chunksæ•°é‡: 38.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 11640.157894736842 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 3440.5 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 130702.0 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 442326.0 chars\n",
      "\n",
      "memalpha_file\n",
      "æ•°æ®æ¡æ•°ï¼š458\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š26.61572052401747\n",
      "å¹³å‡chunksæ•°é‡: 9.106986899563319 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 5056.175497482618 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 1264.0059937664828 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 11503.189956331878 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 46046.52401746725 chars\n",
      "\n",
      "nlu_file\n",
      "æ•°æ®æ¡æ•°ï¼š1\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š100.0\n",
      "å¹³å‡chunksæ•°é‡: 115.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 4063.269565217391 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 1166.7304347826087 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 134060.0 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 467276.0 chars\n",
      "\n",
      "perltqa_file\n",
      "æ•°æ®æ¡æ•°ï¼š4\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š100.0\n",
      "å¹³å‡chunksæ•°é‡: 23.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 2798.1847826086955 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 567.7826086956521 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 13038.25 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 64358.25 chars\n",
      "\n",
      "pubmed_rct_file\n",
      "æ•°æ®æ¡æ•°ï¼š10\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š100.0\n",
      "å¹³å‡chunksæ•°é‡: 10.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 6736.47 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 1673.32 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 16724.2 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 67364.7 chars\n",
      "\n",
      "trec_coarse_file\n",
      "æ•°æ®æ¡æ•°ï¼š1\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š100.0\n",
      "å¹³å‡chunksæ•°é‡: 111.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 4253.7657657657655 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 1114.5585585585586 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 123606.0 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 472168.0 chars\n",
      "\n",
      "trec_fine_file\n",
      "æ•°æ®æ¡æ•°ï¼š1\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š100.0\n",
      "å¹³å‡chunksæ•°é‡: 108.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 4359.027777777777 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 1163.3148148148148 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 125531.0 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 470775.0 chars\n",
      "\n",
      "synthss2_file\n",
      "æ•°æ®æ¡æ•°ï¼š5\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š20.6\n",
      "å¹³å‡chunksæ•°é‡: 2.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 2470.4 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 846.7 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 1693.4 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 4940.8 chars\n",
      "\n",
      "synthss5_file\n",
      "æ•°æ®æ¡æ•°ï¼š4\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š30.75\n",
      "å¹³å‡chunksæ•°é‡: 5.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 2443.6 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 841.45 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 4204.0 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 12218.0 chars\n",
      "\n",
      "synthss10_file\n",
      "æ•°æ®æ¡æ•°ï¼š3\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š34.333333333333336\n",
      "å¹³å‡chunksæ•°é‡: 10.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 2386.233333333333 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 819.9666666666667 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 8191.666666666667 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 23862.333333333332 chars\n",
      "\n",
      "synthss20_file\n",
      "æ•°æ®æ¡æ•°ï¼š3\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š34.0\n",
      "å¹³å‡chunksæ•°é‡: 20.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 2385.7 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 825.2166666666667 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 16486.666666666668 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 47714.0 chars\n",
      "\n",
      "synthss30_file\n",
      "æ•°æ®æ¡æ•°ï¼š3\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š34.333333333333336\n",
      "å¹³å‡chunksæ•°é‡: 30.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 2369.5333333333333 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 826.6666666666666 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 24772.0 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 71086.0 chars\n",
      "\n",
      "synthss40_file\n",
      "æ•°æ®æ¡æ•°ï¼š3\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š35.0\n",
      "å¹³å‡chunksæ•°é‡: 40.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 2357.15 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 820.75 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 32792.666666666664 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 94286.0 chars\n",
      "\n",
      "synthss50_file\n",
      "æ•°æ®æ¡æ•°ï¼š3\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š35.0\n",
      "å¹³å‡chunksæ•°é‡: 50.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 2347.7 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 813.1533333333333 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 40610.333333333336 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 117385.0 chars\n",
      "\n",
      "convomem_file\n",
      "æ•°æ®æ¡æ•°ï¼š107\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š4.186915887850467\n",
      "å¹³å‡chunksæ•°é‡: 339.25233644859816 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 7001.484545454546 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 1535.7162809917356 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 520753.6261682243 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 2375269.9906542054 chars\n",
      "\n",
      "squad_file\n",
      "æ•°æ®æ¡æ•°ï¼š30\n",
      "å¹³å‡é—®é¢˜æ•°é‡ï¼š96.76666666666667\n",
      "å¹³å‡chunksæ•°é‡: 10.0 chunks\n",
      "å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: 4879.54 chars\n",
      "å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: 1056.99 tokens\n",
      "å¹³å‡contextæ€»tokené•¿åº¦: 10560.9 tokens\n",
      "å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: 48795.4 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/.cache/huggingface\"\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", use_fast=True)\n",
    "benchmark_files = {\n",
    "    'hotpotqa_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_hotpotqa_200.json',\n",
    "    'locomo_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_locomo.json',\n",
    "    'longmemeval_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_longmemeval.json',\n",
    "    'msc_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_msc_batch5.json',\n",
    "    'banking77_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_banking77.json',\n",
    "    'booksum_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_booksum.json',\n",
    "    'clinic_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_clinic.json',\n",
    "    'memalpha_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_memalpha.json',\n",
    "    'nlu_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_nlu.json',\n",
    "    'perltqa_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_perltqa.json',\n",
    "    'pubmed_rct_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_pubmed_rct.json',\n",
    "    'trec_coarse_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_trec_coarse.json',\n",
    "    'trec_fine_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_trec_fine.json',\n",
    "    'synthss2_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_synth-ss2.json',\n",
    "    'synthss5_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_synth-ss5.json',\n",
    "    'synthss10_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_synth-ss10.json',\n",
    "    'synthss20_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_synth-ss20.json',\n",
    "    'synthss30_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_synth-ss30.json',\n",
    "    'synthss40_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_synth-ss40.json',\n",
    "    'synthss50_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_synth-ss50.json',\n",
    "    'convomem_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_convomem.json',\n",
    "    'squad_file' : '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_squad.json',\n",
    "}\n",
    "for name, input_file in benchmark_files.items():\n",
    "    print(name)\n",
    "    data = json.load(open(input_file))\n",
    "    if name == 'convomem_file':\n",
    "        for item in data:\n",
    "            item['questions'] = [q for q in item['questions'] if q['category'] == 'user_evidence']\n",
    "    print(\"æ•°æ®æ¡æ•°ï¼š\" + str(len(data)))\n",
    "    print(\"å¹³å‡é—®é¢˜æ•°é‡ï¼š\" + str(sum([len(item['questions']) for item in data]) / len(data)))\n",
    "    print(\"å¹³å‡chunksæ•°é‡: \" + str(np.mean([len(item['chunks']) for item in data])) + ' chunks')\n",
    "    print(\"å¹³å‡æ¯ä¸ªchunkå­—ç¬¦æ•°é‡: \" + str(np.mean([len(chunk) for item in data for chunk in item['chunks']])) + ' chars')\n",
    "    print(\"å¹³å‡æ¯ä¸ªchunk tokenæ•°é‡: \" + str(np.mean([len(tokenizer.tokenize(chunk)) for item in data for chunk in item['chunks']])) + ' tokens')\n",
    "    print(\"å¹³å‡contextæ€»tokené•¿åº¦: \" + str(np.mean([len(tokenizer.tokenize(''.join(item['chunks']))) for item in data])) + ' tokens')\n",
    "    print(\"å¹³å‡contextæ€»å­—ç¬¦é•¿åº¦: \" + str(np.mean([len(''.join(item['chunks'])) for item in data])) + ' chars')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4127e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value(row):\n",
    "    row['extra_info']['num_chunks'] = row['extra_info'].pop('num_docs')\n",
    "    row['extra_info']['question'] = [row['extra_info']['question']]\n",
    "    return row\n",
    "df = df.apply(calculate_value, axis=1)\n",
    "df.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e10a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agent_name'] = 'tool_mem_agent'\n",
    "def get_chunks(context_text, chunk_size):\n",
    "    separator = \"\\n\"\n",
    "    small_chunks = context_text.split(separator)\n",
    "    chunks = []\n",
    "    to_append = \"\"\n",
    "    for chunk in small_chunks:\n",
    "        if len(to_append) + len(chunk) + len(separator) <= chunk_size:\n",
    "            if to_append:\n",
    "                to_append += separator + chunk\n",
    "            else:\n",
    "                to_append = chunk\n",
    "        else:\n",
    "            if to_append:\n",
    "                chunks.append(to_append)\n",
    "            to_append = chunk\n",
    "    if to_append:\n",
    "        chunks.append(to_append)\n",
    "    return chunks\n",
    "\n",
    "def calculate_value(row):\n",
    "    idx = row['extra_info']['index']\n",
    "    filename = f\"/mnt/pfs-guan-ssai/nlu/zhangkehao/verl/memagent/store/memory_store_val{idx}.jsonl\"\n",
    "    chunks = get_chunks(row['context'], 2000)\n",
    "    row['extra_info'][\"tools_kwargs\"] = {\n",
    "        \"memory_add\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_update\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_delete\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_key_retrieve\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_list\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_bm25_retrieve\": {\n",
    "            \"create_kwargs\": {\"chunks\": chunks}\n",
    "        },\n",
    "        \"memory_embedding_retrieve\": {\n",
    "            \"create_kwargs\": {\"chunks\": chunks}\n",
    "        },\n",
    "    }\n",
    "    return row['extra_info']\n",
    "\n",
    "df['extra_info'] = df.apply(calculate_value, axis=1)\n",
    "df.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22903de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem-alpha data_sorce sets: ['pubmed-rct' 'squad' 'hotpotqa' 'perltqa'\n",
      " 'icl_trec_coarse_6600shot_balance' 'icl_nlu_8296shot_balance' 'booksum']\n",
      "\n",
      "MemoryAgentBench data_sorce sets: ['ruler_qa1_197K' 'ruler_qa2_421K' 'longmemeval_s*'\n",
      " 'infbench_sum_eng_shots2' 'icl_banking77_5900shot_balance'\n",
      " 'icl_clinic150_7050shot_balance' 'icl_nlu_8296shot_balance'\n",
      " 'icl_trec_coarse_6600shot_balance' 'icl_trec_fine_6400shot_balance']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "MEMALPHA_PARQUET_PATH = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/Mem-alpha/data/memalpha/test.parquet\"\n",
    "MEMORYAGENTBENCH_PARQUET_PATH = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/Mem-alpha/data/memoryagentbench/test.parquet\"\n",
    "memalpha_df = pd.read_parquet(MEMALPHA_PARQUET_PATH)\n",
    "memoryagentbench_df = pd.read_parquet(MEMORYAGENTBENCH_PARQUET_PATH)\n",
    "print(\"Mem-alpha data_sorce sets:\", memalpha_df['data_source'].unique())\n",
    "print()\n",
    "print(\"MemoryAgentBench data_sorce sets:\", memoryagentbench_df['data_source'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8fd86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'locomo',\n",
       " 'memalpha_booksum',\n",
       " 'memalpha_hotpotqa',\n",
       " 'memalpha_icl_nlu_8296shot_balance',\n",
       " 'memalpha_icl_trec_coarse_6600shot_balance',\n",
       " 'memalpha_perltqa',\n",
       " 'memalpha_pubmed-rct',\n",
       " 'memalpha_squad',\n",
       " 'synth'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "set(pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/synth_train_verl.parquet\")['data_source']) | set(pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/locomo_train_verl.parquet\")['data_source']) | set(pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_train_verl.parquet\")['data_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ffb000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memalpha_booksum',\n",
       " 'memalpha_hotpotqa',\n",
       " 'memalpha_icl_nlu_8296shot_balance',\n",
       " 'memalpha_icl_trec_coarse_6600shot_balance',\n",
       " 'memalpha_perltqa',\n",
       " 'memalpha_pubmed-rct',\n",
       " 'memalpha_squad'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "set(pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_dev_verl.parquet\")['data_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2965878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 140 rows from 7 sources to /mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_dev_verl_small.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_993597/476900753.py:7: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group.sample(n=20, random_state=42) if len(group) >= 20 else group)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_path = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_dev_verl.parquet\"\n",
    "output_path = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_dev_verl_small.parquet\"\n",
    "df = pd.read_parquet(input_path)\n",
    "sampled = (\n",
    "    df.groupby(\"data_source\", group_keys=False)\n",
    "    .apply(lambda group: group.sample(n=20, random_state=42) if len(group) >= 20 else group)\n",
    "    .reset_index(drop=True)\n",
    " )  # sample per source or keep all if fewer than 20\n",
    "sampled.to_parquet(output_path)\n",
    "print(f\"Saved {len(sampled)} rows from {sampled['data_source'].nunique()} sources to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "974e210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 825 JSON files\n",
      "2359440\n",
      "243830\n",
      "1339\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "base_dir = Path(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/ConvoMem/ConvoMem/core_benchmark/pre_mixed_testcases\")\n",
    "json_files = sorted(p.as_posix() for p in base_dir.rglob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "lengths = []\n",
    "for path in json_files:\n",
    "    total_data = json.load(open(path))\n",
    "    for item in total_data:\n",
    "        context = '\\n'.join([mess['text'] for conv in item['conversations'] for mess in conv['messages']])\n",
    "        lengths.append(len(context))\n",
    "\n",
    "print(max(lengths))\n",
    "print(len(lengths))\n",
    "print(len([x for x in lengths if x >  1000000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ef3dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_data[0]['evidenceItems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00fc4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1242 JSON files\n",
      "40590\n",
      "75336\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "base_dir = Path(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/ConvoMem/ConvoMem/core_benchmark/evidence_questions\")\n",
    "json_files = sorted(p.as_posix() for p in base_dir.rglob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "lengths = []\n",
    "for path in json_files:\n",
    "    total_data = json.load(open(path))\n",
    "    for item in total_data['evidence_items']:\n",
    "        context = '\\n'.join([mess['text'] for conv in item['conversations'] for mess in conv['messages']])\n",
    "        lengths.append(len(context))\n",
    "\n",
    "print(max(lengths))\n",
    "print(len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89bb8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, I'm trying to finalize my fitness schedule for the week. Can you help me out?\n",
      "Of course! What activities are you planning to include?\n",
      "Well, climbing is definitely on the agenda. It's such a great workout and stress reliever.\n",
      "Climbing sounds exciting! Do you have a specific day in mind for it?\n",
      "Okay, let's lock in my climbing for the week. I'm going to hit Crux Climbing Center on Tuesday evening.\n",
      "Tuesday evening at Crux Climbing Center it is. Anything else you'd like to add to your schedule?\n",
      "Not at the moment, but I'll let you know if anything else comes up. Thanks for the help!\n",
      "You're welcome! Let me know if you need any more assistance with your schedule.\n",
      "I've been feeling a bit worn out lately. I think I need to plan some downtime.\n",
      "It's important to listen to your body. Do you want to schedule a rest day?\n",
      "Yes, definitely. I need to make sure I schedule a full rest day. My body is telling me it needs one. Let's block out this Wednesday for recovery.\n",
      "Wednesday is now your designated rest day. It's good to have a balance between activity and rest.\n",
      "Absolutely. I always feel better after a proper rest.\n",
      "Great! Let me know if there's anything else you need to plan.\n",
      "I'm thinking about adding some cardio to my routine. What do you think?\n",
      "Cardio is a great addition to any fitness routine. Do you have a specific type of cardio in mind?\n",
      "I should probably squeeze in some light cardio. Let's pencil in a short run for Thursday morning before my meetings start.\n",
      "A short run on Thursday morning sounds perfect. It's a great way to start the day!\n",
      "Exactly, and it won't interfere with my work schedule.\n",
      "That's a smart plan. Let me know if you need help with anything else.\n",
      "I'm trying to fit in another climbing session this week. Any suggestions on when?\n",
      "How about later in the week? That way, you have some time to recover from your first session.\n",
      "For my second climbing session of the week, I want to go to Austin Bouldering Project. Friday after work seems like a good time.\n",
      "Friday after work at Austin Bouldering Project sounds like a great plan. You'll have the weekend to relax afterward.\n",
      "Exactly, and it gives me something to look forward to at the end of the week.\n",
      "That's a great way to end the week. Let me know if there's anything else you need to plan.\n",
      "The weather is supposed to be amazing this weekend. I want to do something outdoors.\n",
      "That sounds like a great idea. Do you have any specific activities in mind?\n",
      "The weather for the weekend looks amazing. I want to take Bowie for a long hike. Let's plan on going to St. Edwards Park on Saturday.\n",
      "A long hike at St. Edwards Park with Bowie on Saturday sounds perfect. It's a great way to enjoy the nice weather.\n",
      "I can't wait. Bowie will love it too!\n",
      "I'm sure he will. Let me know if you need help planning anything else.\n",
      "I want to wrap up my weekend with something relaxing. Any suggestions?\n",
      "How about a yoga class? It's a great way to relax and stretch after a busy week.\n",
      "To round out the weekend, I think a yoga class would be perfect for stretching. I'll book one for Sunday.\n",
      "A yoga class on Sunday sounds like a perfect way to end the weekend. You'll feel refreshed for the upcoming week.\n",
      "Exactly what I need. Thanks for the suggestion!\n",
      "You're welcome! Let me know if there's anything else you need help with.\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee508981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4094"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalphafull_train_verl.parquet\")\n",
    "len(df)\n",
    "# df = pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_dev_agent_loop.parquet\")\n",
    "# def reformat(row):\n",
    "#     row['extra_info']['question'] = [row['extra_info']['question']]\n",
    "#     return row\n",
    "# df = df.apply(reformat, axis=1)\n",
    "# df.to_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_dev_agent_loop.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d8ed12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['memalpha_icl_nlu_8296shot_balance', 'memalpha_hotpotqa',\n",
       "       'memalpha_squad', 'memalpha_icl_trec_coarse_6600shot_balance',\n",
       "       'memalpha_perltqa', 'memalpha_booksum', 'memalpha_pubmed-rct'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data_source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9ef2db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['memalpha_hotpotqa', 'memalpha_squad', 'memalpha_booksum',\n",
       "       'memalpha_icl_nlu_8296shot_balance',\n",
       "       'memalpha_icl_trec_coarse_6600shot_balance', 'memalpha_pubmed-rct',\n",
       "       'memalpha_perltqa'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data_source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0795391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hotpotqa': 100,\n",
       "         'squad': 100,\n",
       "         'booksum': 100,\n",
       "         'pubmed-rct': 90,\n",
       "         'icl_trec_coarse_6600shot_balance': 51,\n",
       "         'lme_train': 50,\n",
       "         'icl_nlu_8296shot_balance': 49,\n",
       "         'perltqa': 27})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for source in df['data_source']:\n",
    "    counter[source] += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc84df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(counter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665be89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_source</th>\n",
       "      <th>prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>reward_model</th>\n",
       "      <th>extra_info</th>\n",
       "      <th>agent_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>memalpha_icl_nlu_8296shot_balance</td>\n",
       "      <td>[{'content': ['Sentence: fight me in street fi...</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>{'ground_truth': ['43', '66', '5', '14', '15',...</td>\n",
       "      <td>{'index': 0, 'num_chunks': 10, 'original_sampl...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>memalpha_icl_nlu_8296shot_balance</td>\n",
       "      <td>[{'content': ['Sentence: what do you want to d...</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>{'ground_truth': ['66', '38', '5', '5', '5', '...</td>\n",
       "      <td>{'index': 1, 'num_chunks': 10, 'original_sampl...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>memalpha_icl_nlu_8296shot_balance</td>\n",
       "      <td>[{'content': ['Sentence: what are the bbc poll...</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>{'ground_truth': ['36', '38', '14', '18', '65'...</td>\n",
       "      <td>{'index': 2, 'num_chunks': 10, 'original_sampl...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>memalpha_icl_nlu_8296shot_balance</td>\n",
       "      <td>[{'content': ['Sentence: add eggs to my grocer...</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>{'ground_truth': ['57', '66', '21', '12', '29'...</td>\n",
       "      <td>{'index': 3, 'num_chunks': 10, 'original_sampl...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>memalpha_icl_nlu_8296shot_balance</td>\n",
       "      <td>[{'content': ['Sentence: send an email to emma...</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>{'ground_truth': ['50', '36', '23', '17', '41'...</td>\n",
       "      <td>{'index': 4, 'num_chunks': 10, 'original_sampl...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>memalpha_pubmed-rct</td>\n",
       "      <td>[{'content': ['Sentence: It appears that poten...</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>{'ground_truth': ['1', '4', '2', '3', '4', '0'...</td>\n",
       "      <td>{'index': 3429, 'num_chunks': 10, 'original_sa...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>memalpha_pubmed-rct</td>\n",
       "      <td>[{'content': ['Sentence: Oxygen extraction rat...</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>{'ground_truth': ['4', '0', '2', '4', '2', '4'...</td>\n",
       "      <td>{'index': 3430, 'num_chunks': 10, 'original_sa...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>memalpha_pubmed-rct</td>\n",
       "      <td>[{'content': ['Sentence: The differences betwe...</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>{'ground_truth': ['4', '3', '4', '2', '0', '2'...</td>\n",
       "      <td>{'index': 3431, 'num_chunks': 10, 'original_sa...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>memalpha_pubmed-rct</td>\n",
       "      <td>[{'content': ['Sentence: We studied 137 patien...</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>{'ground_truth': ['2', '1', '0', '2', '1', '4'...</td>\n",
       "      <td>{'index': 3432, 'num_chunks': 10, 'original_sa...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>memalpha_pubmed-rct</td>\n",
       "      <td>[{'content': ['Sentence: Invasive fungal infec...</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>{'ground_truth': ['3', '4', '2', '4', '4', '0'...</td>\n",
       "      <td>{'index': 3433, 'num_chunks': 10, 'original_sa...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3434 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            data_source  \\\n",
       "0     memalpha_icl_nlu_8296shot_balance   \n",
       "1     memalpha_icl_nlu_8296shot_balance   \n",
       "2     memalpha_icl_nlu_8296shot_balance   \n",
       "3     memalpha_icl_nlu_8296shot_balance   \n",
       "4     memalpha_icl_nlu_8296shot_balance   \n",
       "...                                 ...   \n",
       "3429                memalpha_pubmed-rct   \n",
       "3430                memalpha_pubmed-rct   \n",
       "3431                memalpha_pubmed-rct   \n",
       "3432                memalpha_pubmed-rct   \n",
       "3433                memalpha_pubmed-rct   \n",
       "\n",
       "                                                 prompt  \\\n",
       "0     [{'content': ['Sentence: fight me in street fi...   \n",
       "1     [{'content': ['Sentence: what do you want to d...   \n",
       "2     [{'content': ['Sentence: what are the bbc poll...   \n",
       "3     [{'content': ['Sentence: add eggs to my grocer...   \n",
       "4     [{'content': ['Sentence: send an email to emma...   \n",
       "...                                                 ...   \n",
       "3429  [{'content': ['Sentence: It appears that poten...   \n",
       "3430  [{'content': ['Sentence: Oxygen extraction rat...   \n",
       "3431  [{'content': ['Sentence: The differences betwe...   \n",
       "3432  [{'content': ['Sentence: We studied 137 patien...   \n",
       "3433  [{'content': ['Sentence: Invasive fungal infec...   \n",
       "\n",
       "                                                context  \\\n",
       "0     I will provide you with classification trainin...   \n",
       "1     I will provide you with classification trainin...   \n",
       "2     I will provide you with classification trainin...   \n",
       "3     I will provide you with classification trainin...   \n",
       "4     I will provide you with classification trainin...   \n",
       "...                                                 ...   \n",
       "3429  You are a scientific document classifier. I wi...   \n",
       "3430  You are a scientific document classifier. I wi...   \n",
       "3431  You are a scientific document classifier. I wi...   \n",
       "3432  You are a scientific document classifier. I wi...   \n",
       "3433  You are a scientific document classifier. I wi...   \n",
       "\n",
       "                                           reward_model  \\\n",
       "0     {'ground_truth': ['43', '66', '5', '14', '15',...   \n",
       "1     {'ground_truth': ['66', '38', '5', '5', '5', '...   \n",
       "2     {'ground_truth': ['36', '38', '14', '18', '65'...   \n",
       "3     {'ground_truth': ['57', '66', '21', '12', '29'...   \n",
       "4     {'ground_truth': ['50', '36', '23', '17', '41'...   \n",
       "...                                                 ...   \n",
       "3429  {'ground_truth': ['1', '4', '2', '3', '4', '0'...   \n",
       "3430  {'ground_truth': ['4', '0', '2', '4', '2', '4'...   \n",
       "3431  {'ground_truth': ['4', '3', '4', '2', '0', '2'...   \n",
       "3432  {'ground_truth': ['2', '1', '0', '2', '1', '4'...   \n",
       "3433  {'ground_truth': ['3', '4', '2', '4', '4', '0'...   \n",
       "\n",
       "                                             extra_info      agent_name  \n",
       "0     {'index': 0, 'num_chunks': 10, 'original_sampl...  tool_mem_agent  \n",
       "1     {'index': 1, 'num_chunks': 10, 'original_sampl...  tool_mem_agent  \n",
       "2     {'index': 2, 'num_chunks': 10, 'original_sampl...  tool_mem_agent  \n",
       "3     {'index': 3, 'num_chunks': 10, 'original_sampl...  tool_mem_agent  \n",
       "4     {'index': 4, 'num_chunks': 10, 'original_sampl...  tool_mem_agent  \n",
       "...                                                 ...             ...  \n",
       "3429  {'index': 3429, 'num_chunks': 10, 'original_sa...  tool_mem_agent  \n",
       "3430  {'index': 3430, 'num_chunks': 10, 'original_sa...  tool_mem_agent  \n",
       "3431  {'index': 3431, 'num_chunks': 10, 'original_sa...  tool_mem_agent  \n",
       "3432  {'index': 3432, 'num_chunks': 10, 'original_sa...  tool_mem_agent  \n",
       "3433  {'index': 3433, 'num_chunks': 10, 'original_sa...  tool_mem_agent  \n",
       "\n",
       "[3434 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca22271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'memalpha_squad': 957,\n",
       "         'memalpha_pubmed-rct': 900,\n",
       "         'memalpha_icl_trec_coarse_6600shot_balance': 510,\n",
       "         'memalpha_icl_nlu_8296shot_balance': 490,\n",
       "         'memalpha_perltqa': 270,\n",
       "         'memalpha_hotpotqa': 207,\n",
       "         'memalpha_booksum': 100})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for source in df['data_source']:\n",
    "    counter[source] += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e224c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhangkehao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
