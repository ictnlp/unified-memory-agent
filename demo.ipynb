{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_dev.parquet\"\n",
    "df = pd.read_parquet(file_path)\n",
    "df['agent_name'] = 'mem_agent'\n",
    "df.to_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_dev_agent_loop.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2bdbcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据条数：500\n",
      "正确率：0.382\n",
      "knowledge-update 0.4358974358974359\n",
      "34 / 78\n",
      "single-session-user 0.4142857142857143\n",
      "29 / 70\n",
      "temporal-reasoning 0.46616541353383456\n",
      "62 / 133\n",
      "single-session-preference 0.6\n",
      "18 / 30\n",
      "single-session-assistant 0.17857142857142858\n",
      "10 / 56\n",
      "multi-session 0.2857142857142857\n",
      "38 / 133\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/HippoRAG/longmemeval_s_r1_log_no_longcontext_recall20_eval_res.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/longmemeval_s/Qwen3-8B-5k-1k-infty_eval_res.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/processing_log_qwen3-8b_1758104883.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/processing_log_qwen3-8b_faster_1758253073.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/results/qwen3-8b/longmemeval/evaluated_concat.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/results/qwen3-8b/longmemeval_oracle/evaluated_concat.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/ReMe/cookbook/simple_demo/longmemeval/evaluated_unknown.jsonl\"\n",
    "data = [json.loads(line) for line in open(datapath)]\n",
    "\n",
    "original_datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/data/longmemeval_s.json\"\n",
    "original_data = json.load(open(original_datapath))\n",
    "for d in data:\n",
    "    for od in original_data:\n",
    "        question_id = d.get(\"question_id\", d.get(\"qid\"))\n",
    "        if \"longmemeval_\" in question_id:\n",
    "            question_id = question_id[len(\"longmemeval_\"):-2]\n",
    "        if od[\"question_id\"] == question_id:\n",
    "            d[\"question_type\"] = od[\"question_type\"]\n",
    "            break\n",
    "\n",
    "# datapath = \"longmemeval_s_r1_log_no_longcontext_recall20.json\"\n",
    "# data = json.load(open(datapath))\n",
    "print(\"数据条数：\" + str(len([x for x in data if x.get('qid') is not None])))\n",
    "print(\"正确率：\" + str(len([x for x in data if x[\"metric\"].get('llm_score')]) / len([x for x in data if x[\"metric\"].get('llm_score') is not None])))\n",
    "\n",
    "for tp in [\"knowledge-update\", \"single-session-user\", \"temporal-reasoning\", \"single-session-preference\", \"single-session-assistant\", \"multi-session\"]:\n",
    "    if len([x for x in data if x.get('question_type')==tp]) == 0:\n",
    "        print(tp, 0)\n",
    "    else:\n",
    "        print(tp, len([x for x in data if x[\"metric\"].get('llm_score') and x.get('question_type')==tp]) / len([x for x in data if x.get('question_type')==tp]))\n",
    "        print(f\"{len([x for x in data if x[\"metric\"].get('llm_score') and x.get('question_type')==tp])} / {len([x for x in data if x.get('question_type')==tp])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74531ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据条数：1986\n",
      "正确率：0.28751258811681774\n",
      "multi hop 0.09574468085106383\n",
      "27 / 282\n",
      "temporal 0.07476635514018691\n",
      "24 / 321\n",
      "open domain 0.21875\n",
      "21 / 96\n",
      "single hop 0.06539833531510107\n",
      "55 / 841\n",
      "adversarial 0.9955156950672646\n",
      "444 / 446\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/results/qwen3-8b/locomo/evaluated_concat.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/locomo_results_qwen3-8b_faster_1759028934.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/locomo_results_1759036272.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/locomo/Qwen3-8B-5k-1k-infty.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/HippoRAG/locomo_hippo_qwen_recall42.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/HippoRAG/locomo_hippo_qwen_recall20.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/ReMe/cookbook/simple_demo/locomo/evaluated_unknown.jsonl\"\n",
    "data = [json.loads(line) for line in open(datapath)]\n",
    "# data = sum([d[\"queries_solutions\"] for d in data], start=[])\n",
    "\n",
    "original_datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/raw/locomo10.json\"\n",
    "original_data = json.load(open(original_datapath))\n",
    "\n",
    "for d in data:\n",
    "    for od in original_data:\n",
    "        if od['sample_id'] in d['qid']:\n",
    "            q_index = int(d['qid'].split('_')[2])\n",
    "            d['question_type'] = od['qa'][q_index]['category']\n",
    "\n",
    "for d in data:\n",
    "    # d['question_type'] = d['category']\n",
    "    d[\"gpt4o_score\"] =  d[\"metric\"].get('llm_score')\n",
    "\n",
    "print(\"数据条数：\" + str(len(data)))\n",
    "print(\"正确率：\" + str(len([x for x in data if x[\"gpt4o_score\"]]) / len([x for x in data if x[\"gpt4o_score\"] is not None])))\n",
    "\n",
    "question_type_map = {\n",
    "    1: \"multi hop\",\n",
    "    2: \"temporal\",\n",
    "    3: \"open domain\",\n",
    "    4: \"single hop\",\n",
    "    5: \"adversarial\"\n",
    "}\n",
    "\n",
    "\n",
    "for tp in range(1,6):\n",
    "    if len([x for x in data if x.get('question_type')==tp]) == 0:\n",
    "        print(question_type_map[tp], 0)\n",
    "    else:\n",
    "        print(question_type_map[tp], len([x for x in data if x[\"gpt4o_score\"] and x.get('question_type')==tp]) / len([x for x in data if x.get('question_type')==tp]))\n",
    "        print(f\"{len([x for x in data if x[\"gpt4o_score\"] and x.get('question_type')==tp])} / {len([x for x in data if x.get('question_type')==tp])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "from openai import OpenAI, RateLimitError\n",
    "import uuid\n",
    "import backoff\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm as atqdm\n",
    "\n",
    "from config import MODEL_NAME, API_CONFIG_LOCAL\n",
    "from openai import OpenAI\n",
    "\n",
    "async def gpt4o_evaluation(question, correct_answer, model_response, question_type):\n",
    "    \"\"\"\n",
    "    Use GPT-4o to evaluate the model response\n",
    "    Based on reprocess_res.py\n",
    "    \"\"\"\n",
    "\n",
    "    API_CONFIG_LOCAL[\"base_url\"] = \"http://172.24.139.15:8000/v1\"\n",
    "    openai_client = OpenAI(**API_CONFIG_LOCAL)\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        RateLimitError,\n",
    "        max_tries=16,\n",
    "        max_time=300,\n",
    "        jitter=backoff.full_jitter\n",
    "    )\n",
    "    async def llm_response(messages):\n",
    "        # 使用 asyncio.to_thread 将同步调用转为异步\n",
    "        res = await asyncio.to_thread(\n",
    "            openai_client.chat.completions.create,\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "        response_message = res.choices[0].message.content\n",
    "        if \"</think>\" in response_message:\n",
    "            response_message = response_message.split(\"</think>\")[-1].strip()\n",
    "        # print(response_message)\n",
    "        return response_message\n",
    "    \n",
    "    def get_anscheck_prompt(task, question, answer, response):\n",
    "        LLM_TEMPLATE = \"I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no.\\n\\nQuestion: {}\\n\\nCorrect Answer: {}\\n\\nModel Response: {}\\n\\nIs the model response correct? Answer yes or no only.\"\n",
    "        prompt = LLM_TEMPLATE.format(question, answer, response)\n",
    "        return prompt\n",
    "    \n",
    "    async def anscheck(task, question, answer, response):\n",
    "        prompt = get_anscheck_prompt(task, question, answer, response)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        res = await llm_response(messages)\n",
    "        return 'yes' in res.lower() and not 'no' in res.lower()\n",
    "    \n",
    "    return await anscheck(question_type, question, correct_answer, model_response)\n",
    "\n",
    "async def process_item(d, output_file, semaphore, file_lock):\n",
    "    async with semaphore:\n",
    "        score = await gpt4o_evaluation(d[\"input\"], d[\"answer\"], d[\"response\"], d[\"question_type\"])\n",
    "        d[\"gpt4o_score\"] = score\n",
    "        \n",
    "        # 使用文件锁确保并发写入安全\n",
    "        async with file_lock:\n",
    "            with open(output_file, \"a\") as f:\n",
    "                f.write(json.dumps(d) + '\\n')\n",
    "        \n",
    "        return d\n",
    "\n",
    "async def main():\n",
    "    data_path = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/locomo/Qwen3-8B-5k-1k-infty.jsonl\"\n",
    "    data = [json.loads(line) for line in open(data_path)]\n",
    "    \n",
    "    output_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/locomo/Qwen3-8B-5k-1k-infty_.jsonl\"\n",
    "    \n",
    "    # 创建信号量控制并发数\n",
    "    semaphore = asyncio.Semaphore(20)  # 可调整并发数\n",
    "    \n",
    "    # 创建文件锁，确保并发写入安全\n",
    "    file_lock = asyncio.Lock()\n",
    "    \n",
    "    # 创建所有任务\n",
    "    tasks = [process_item(d, output_file, semaphore, file_lock) for d in data]\n",
    "    \n",
    "    # 使用异步进度条执行\n",
    "    results = await atqdm.gather(*tasks, desc=\"Processing\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # asyncio.run(main())\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348906d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id=None, choices=None, created=None, model=None, object=None, service_tier=None, system_fingerprint=None, usage=None, code=100049, msg='接口验证失败,请检查Token是否正确', success=False, data={})\n"
     ]
    }
   ],
   "source": [
    "from config import MODEL_NAME, API_CONFIG_LOCAL, API_CONFIG\n",
    "from openai import OpenAI\n",
    "API_CONFIG_LOCAL[\"base_url\"] = \"http://172.24.139.15:8000/v1\"\n",
    "openai_client = OpenAI(**API_CONFIG_LOCAL)\n",
    "openai_client = OpenAI(**API_CONFIG)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hello\"\n",
    "    }\n",
    "]\n",
    "res = openai_client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    logprobs=1\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc7d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<think>',\n",
       " '\\n',\n",
       " 'Okay',\n",
       " ',',\n",
       " ' the',\n",
       " ' user',\n",
       " ' said',\n",
       " ' \"',\n",
       " 'hello',\n",
       " '\".',\n",
       " ' I',\n",
       " ' need',\n",
       " ' to',\n",
       " ' respond',\n",
       " ' appropriately',\n",
       " '.',\n",
       " ' Since',\n",
       " ' it',\n",
       " \"'s\",\n",
       " ' a',\n",
       " ' greeting',\n",
       " ',',\n",
       " ' I',\n",
       " ' should',\n",
       " ' acknowledge',\n",
       " ' it',\n",
       " ' and',\n",
       " ' offer',\n",
       " ' assistance',\n",
       " '.',\n",
       " ' Let',\n",
       " ' me',\n",
       " ' make',\n",
       " ' sure',\n",
       " ' the',\n",
       " ' response',\n",
       " ' is',\n",
       " ' friendly',\n",
       " ' and',\n",
       " ' open',\n",
       " '-ended',\n",
       " '.',\n",
       " ' Maybe',\n",
       " ' ask',\n",
       " ' how',\n",
       " ' I',\n",
       " ' can',\n",
       " ' help',\n",
       " ' them',\n",
       " ' today',\n",
       " '.',\n",
       " ' Keep',\n",
       " ' it',\n",
       " ' simple',\n",
       " ' and',\n",
       " ' welcoming',\n",
       " '.\\n',\n",
       " '</think>',\n",
       " '\\n\\n',\n",
       " 'Hello',\n",
       " '!',\n",
       " ' How',\n",
       " ' can',\n",
       " ' I',\n",
       " ' assist',\n",
       " ' you',\n",
       " ' today',\n",
       " '?',\n",
       " ' �',\n",
       " '�',\n",
       " '<|im_end|>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.token for x in res.choices[0].logprobs.content]\n",
    "res.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5574cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_context() -> str:\n",
    "    \"\"\"Load contextual information fed into ``ToolMemoryAgentLoop``.\n",
    "\n",
    "    Projects can override this helper to supply domain-specific context. By default it\n",
    "    returns the value from ``TOOL_MEM_AGENT_TEST_CONTEXT`` (or an empty string).\n",
    "    \"\"\"\n",
    "\n",
    "    # return os.environ.get(\"TOOL_MEM_AGENT_TEST_CONTEXT\", \"I like to eat bananas.\")\n",
    "    raw = json.load(open(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/zep/benchmarks/longmemeval/data/longmemeval_s.json\"))\n",
    "    item0 = raw[0]\n",
    "    context = \"Below is a conversation between User and Assistant.\\n\\n\"\n",
    "    for idx, session in enumerate(item0[\"haystack_sessions\"]):\n",
    "        session_date = item0[\"haystack_dates\"][idx]\n",
    "        session_conv = ''\n",
    "        for turn in session:\n",
    "            role = \"User\" if turn['role'] == 'user' else \"Assistant\"\n",
    "            turn_text = f'{role} said, \"{turn[\"content\"]}\"'\n",
    "            turn_text += '\\n'\n",
    "            session_conv += turn_text\n",
    "        if not session_conv:\n",
    "            session_conv = \"NO CONVERSATION\"\n",
    "        query_conv = f'DATE: {session_date}\\nCONVERSATION:\\n{session_conv}\\n\\n'\n",
    "        context += query_conv\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat\n",
    "import json\n",
    "input_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/verl/base_result.jsonl\"\n",
    "output_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/verl/base_result_reformat.jsonl\"\n",
    "original_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/zep/benchmarks/longmemeval/data/longmemeval_s.json\"\n",
    "data = [json.loads(line) for line in open(input_file)]\n",
    "original_data = json.load(open(original_file))\n",
    "with open(output_file, \"w\") as f:\n",
    "    for d in data:\n",
    "        original_d = original_data[d['idx']]\n",
    "        response = d[\"response\"]\n",
    "        if \"\\\\boxed{\" in response and \"}\" in response.split(\"\\\\boxed{\")[-1]:\n",
    "            response = response.split(\"\\\\boxed{\")[-1]\n",
    "            response = response[:response.rindex(\"}\")]\n",
    "        new_d = {\n",
    "            \"qid\": \"longmemeval_\" + original_d[\"question_id\"] + \"_0\",\n",
    "            \"query\": original_d[\"question\"],\n",
    "            \"expected_answer\": original_d[\"answer\"],\n",
    "            \"response\": response\n",
    "        }\n",
    "        f.write(json.dumps(new_d) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memalpha\n",
      "1461 records\n",
      "9.89596167008898 chunks\n",
      "4046.8266011896526 chars\n",
      "\n",
      "locomo\n",
      "20 records\n",
      "19.0 chunks\n",
      "3942.8947368421054 chars\n",
      "\n",
      "hotpotqa\n",
      "8192 records\n",
      "61.06103515625 chunks\n",
      "1701.0528355977065 chars\n",
      "\n",
      "synth\n",
      "89 records\n",
      "10.0 chunks\n",
      "6008.816853932584 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "memalpha_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_dev_verl.parquet'\n",
    "locomo_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/locomo_train_verl.parquet'\n",
    "hpq_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_train_mem_agent_loop_chunk2000.parquet'\n",
    "sync_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/synth_train_verl.parquet'\n",
    "for input_file, name in zip([memalpha_file, locomo_file, hpq_file, sync_file], ['memalpha', 'locomo', 'hotpotqa', 'synth']):\n",
    "    print(name)\n",
    "    df = pd.read_parquet(input_file)\n",
    "    print(len(df), 'records')\n",
    "    print(np.mean([len([len(x) for x in y['tools_kwargs']['memory_bm25_retrieve']['create_kwargs']['chunks']]) for y in df['extra_info']]), 'chunks') # 平均chunks数量\n",
    "    print(np.mean([len(x) for y in df['extra_info'] for x in y['tools_kwargs']['memory_bm25_retrieve']['create_kwargs']['chunks']]), 'chars\\n') # 平均每个chunk字符数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18ef485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotpotqa\n",
      "数据条数：128\n",
      "平均问题数量：1.0\n",
      "平均chunks数量: 200.0 chunks\n",
      "平均每个chunk字符数量: 570.0202734375 chars\n",
      "平均每个chunk token数量: 140.5790625 tokens\n",
      "平均context总字符长度: 114004.0546875 chars\n",
      "\n",
      "locomo\n",
      "数据条数：10\n",
      "平均问题数量：198.6\n",
      "平均chunks数量: 27.2 chunks\n",
      "平均每个chunk字符数量: 3398.1507352941176 chars\n",
      "平均每个chunk token数量: 814.7279411764706 tokens\n",
      "平均context总字符长度: 92429.7 chars\n",
      "\n",
      "longmemeval\n",
      "数据条数：500\n",
      "平均问题数量：1.0\n",
      "平均chunks数量: 47.764 chunks\n",
      "平均每个chunk字符数量: 10521.834394104346 chars\n",
      "平均每个chunk token数量: 2258.246838623231 tokens\n",
      "平均context总字符长度: 502564.898 chars\n",
      "\n",
      "msc\n",
      "数据条数：100\n",
      "平均问题数量：5.0\n",
      "平均chunks数量: 25.0 chunks\n",
      "平均每个chunk字符数量: 1563.9144 chars\n",
      "平均每个chunk token数量: 394.1004 tokens\n",
      "平均context总字符长度: 39097.86 chars\n",
      "\n",
      "banking77\n",
      "数据条数：1\n",
      "平均问题数量：100.0\n",
      "平均chunks数量: 111.0 chunks\n",
      "平均每个chunk字符数量: 4279.54954954955 chars\n",
      "平均每个chunk token数量: 1150.2522522522522 tokens\n",
      "平均context总字符长度: 475030.0 chars\n",
      "\n",
      "booksum\n",
      "数据条数：155\n",
      "平均问题数量：1.0\n",
      "平均chunks数量: 8.135483870967741 chunks\n",
      "平均每个chunk字符数量: 7643.934179222839 chars\n",
      "平均每个chunk token数量: 1914.3243457573355 tokens\n",
      "平均context总字符长度: 62187.10322580645 chars\n",
      "\n",
      "clinic\n",
      "数据条数：1\n",
      "平均问题数量：100.0\n",
      "平均chunks数量: 38.0 chunks\n",
      "平均每个chunk字符数量: 11640.157894736842 chars\n",
      "平均每个chunk token数量: 3440.5 tokens\n",
      "平均context总字符长度: 442326.0 chars\n",
      "\n",
      "memalpha\n",
      "数据条数：458\n",
      "平均问题数量：26.61572052401747\n",
      "平均chunks数量: 9.106986899563319 chunks\n",
      "平均每个chunk字符数量: 5056.175497482618 chars\n",
      "平均每个chunk token数量: 1264.0059937664828 tokens\n",
      "平均context总字符长度: 46046.52401746725 chars\n",
      "\n",
      "nlu\n",
      "数据条数：1\n",
      "平均问题数量：100.0\n",
      "平均chunks数量: 115.0 chunks\n",
      "平均每个chunk字符数量: 4063.269565217391 chars\n",
      "平均每个chunk token数量: 1166.7304347826087 tokens\n",
      "平均context总字符长度: 467276.0 chars\n",
      "\n",
      "perltqa\n",
      "数据条数：4\n",
      "平均问题数量：100.0\n",
      "平均chunks数量: 23.0 chunks\n",
      "平均每个chunk字符数量: 2798.1847826086955 chars\n",
      "平均每个chunk token数量: 567.7826086956521 tokens\n",
      "平均context总字符长度: 64358.25 chars\n",
      "\n",
      "pubmed_rct\n",
      "数据条数：10\n",
      "平均问题数量：100.0\n",
      "平均chunks数量: 10.0 chunks\n",
      "平均每个chunk字符数量: 6736.47 chars\n",
      "平均每个chunk token数量: 1673.32 tokens\n",
      "平均context总字符长度: 67364.7 chars\n",
      "\n",
      "trec_coarse\n",
      "数据条数：1\n",
      "平均问题数量：100.0\n",
      "平均chunks数量: 111.0 chunks\n",
      "平均每个chunk字符数量: 4253.7657657657655 chars\n",
      "平均每个chunk token数量: 1114.5585585585586 tokens\n",
      "平均context总字符长度: 472168.0 chars\n",
      "\n",
      "trec_fine\n",
      "数据条数：1\n",
      "平均问题数量：100.0\n",
      "平均chunks数量: 108.0 chunks\n",
      "平均每个chunk字符数量: 4359.027777777777 chars\n",
      "平均每个chunk token数量: 1163.3148148148148 tokens\n",
      "平均context总字符长度: 470775.0 chars\n",
      "\n",
      "synth10\n",
      "数据条数：1\n",
      "平均问题数量：81.0\n",
      "平均chunks数量: 10.0 chunks\n",
      "平均每个chunk字符数量: 3072.2 chars\n",
      "平均每个chunk token数量: 1064.8 tokens\n",
      "平均context总字符长度: 30722.0 chars\n",
      "\n",
      "synth50\n",
      "数据条数：1\n",
      "平均问题数量：93.0\n",
      "平均chunks数量: 50.0 chunks\n",
      "平均每个chunk字符数量: 5957.12 chars\n",
      "平均每个chunk token数量: 1978.2 tokens\n",
      "平均context总字符长度: 297856.0 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/.cache/huggingface\"\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", use_fast=True)\n",
    "hotpotqa_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_hotpotqa_200.json'\n",
    "locomo_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_locomo.json'\n",
    "longmemeval_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_longmemeval.json'\n",
    "msc_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_msc_batch5.json'\n",
    "banking77_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_banking77.json'\n",
    "booksum_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_booksum.json'\n",
    "clinic_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_clinic.json'\n",
    "memalpha_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_memalpha.json'\n",
    "nlu_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_nlu.json'\n",
    "perltqa_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_perltqa.json'\n",
    "pubmed_rct_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_pubmed_rct.json'\n",
    "trec_coarse_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_trec_coarse.json'\n",
    "trec_fine_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_trec_fine.json'\n",
    "synth10_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_synth-s10.json'\n",
    "synth50_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_synth-s50.json'\n",
    "for input_file, name in zip([hotpotqa_file, locomo_file, longmemeval_file, msc_file, banking77_file, booksum_file, clinic_file, memalpha_file, nlu_file, perltqa_file, pubmed_rct_file, trec_coarse_file, trec_fine_file, synth10_file, synth50_file], ['hotpotqa', 'locomo', 'longmemeval', 'msc', 'banking77', 'booksum', 'clinic', 'memalpha', 'nlu', 'perltqa', 'pubmed_rct', 'trec_coarse', 'trec_fine', 'synth10', 'synth50']):\n",
    "    print(name)\n",
    "    data = json.load(open(input_file))\n",
    "    print(\"数据条数：\" + str(len(data)))\n",
    "    print(\"平均问题数量：\" + str(sum([len(item['questions']) for item in data]) / len(data)))\n",
    "    print(\"平均chunks数量: \" + str(np.mean([len(item['chunks']) for item in data])) + ' chunks')\n",
    "    print(\"平均每个chunk字符数量: \" + str(np.mean([len(chunk) for item in data for chunk in item['chunks']])) + ' chars')\n",
    "    print(\"平均每个chunk token数量: \" + str(np.mean([len(tokenizer.tokenize(chunk)) for item in data for chunk in item['chunks']])) + ' tokens')\n",
    "    print(\"平均context总字符长度: \" + str(np.mean([len(''.join(item['chunks'])) for item in data])) + ' chars')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4127e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value(row):\n",
    "    row['extra_info']['num_chunks'] = row['extra_info'].pop('num_docs')\n",
    "    row['extra_info']['question'] = [row['extra_info']['question']]\n",
    "    return row\n",
    "df = df.apply(calculate_value, axis=1)\n",
    "df.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e10a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agent_name'] = 'tool_mem_agent'\n",
    "def get_chunks(context_text, chunk_size):\n",
    "    separator = \"\\n\"\n",
    "    small_chunks = context_text.split(separator)\n",
    "    chunks = []\n",
    "    to_append = \"\"\n",
    "    for chunk in small_chunks:\n",
    "        if len(to_append) + len(chunk) + len(separator) <= chunk_size:\n",
    "            if to_append:\n",
    "                to_append += separator + chunk\n",
    "            else:\n",
    "                to_append = chunk\n",
    "        else:\n",
    "            if to_append:\n",
    "                chunks.append(to_append)\n",
    "            to_append = chunk\n",
    "    if to_append:\n",
    "        chunks.append(to_append)\n",
    "    return chunks\n",
    "\n",
    "def calculate_value(row):\n",
    "    idx = row['extra_info']['index']\n",
    "    filename = f\"/mnt/pfs-guan-ssai/nlu/zhangkehao/verl/memagent/store/memory_store_val{idx}.jsonl\"\n",
    "    chunks = get_chunks(row['context'], 2000)\n",
    "    row['extra_info'][\"tools_kwargs\"] = {\n",
    "        \"memory_add\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_update\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_delete\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_key_retrieve\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_list\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_bm25_retrieve\": {\n",
    "            \"create_kwargs\": {\"chunks\": chunks}\n",
    "        },\n",
    "        \"memory_embedding_retrieve\": {\n",
    "            \"create_kwargs\": {\"chunks\": chunks}\n",
    "        },\n",
    "    }\n",
    "    return row['extra_info']\n",
    "\n",
    "df['extra_info'] = df.apply(calculate_value, axis=1)\n",
    "df.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22903de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem-alpha data_sorce sets: ['pubmed-rct' 'squad' 'hotpotqa' 'perltqa'\n",
      " 'icl_trec_coarse_6600shot_balance' 'icl_nlu_8296shot_balance' 'booksum']\n",
      "\n",
      "MemoryAgentBench data_sorce sets: ['ruler_qa1_197K' 'ruler_qa2_421K' 'longmemeval_s*'\n",
      " 'infbench_sum_eng_shots2' 'icl_banking77_5900shot_balance'\n",
      " 'icl_clinic150_7050shot_balance' 'icl_nlu_8296shot_balance'\n",
      " 'icl_trec_coarse_6600shot_balance' 'icl_trec_fine_6400shot_balance']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "MEMALPHA_PARQUET_PATH = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/Mem-alpha/data/memalpha/test.parquet\"\n",
    "MEMORYAGENTBENCH_PARQUET_PATH = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/Mem-alpha/data/memoryagentbench/test.parquet\"\n",
    "memalpha_df = pd.read_parquet(MEMALPHA_PARQUET_PATH)\n",
    "memoryagentbench_df = pd.read_parquet(MEMORYAGENTBENCH_PARQUET_PATH)\n",
    "print(\"Mem-alpha data_sorce sets:\", memalpha_df['data_source'].unique())\n",
    "print()\n",
    "print(\"MemoryAgentBench data_sorce sets:\", memoryagentbench_df['data_source'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8fd86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'locomo',\n",
       " 'memalpha_booksum',\n",
       " 'memalpha_hotpotqa',\n",
       " 'memalpha_icl_nlu_8296shot_balance',\n",
       " 'memalpha_icl_trec_coarse_6600shot_balance',\n",
       " 'memalpha_perltqa',\n",
       " 'memalpha_pubmed-rct',\n",
       " 'memalpha_squad',\n",
       " 'synth'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "set(pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/synth_train_verl.parquet\")['data_source']) | set(pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/locomo_train_verl.parquet\")['data_source']) | set(pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_train_verl.parquet\")['data_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ffb000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memalpha_booksum',\n",
       " 'memalpha_hotpotqa',\n",
       " 'memalpha_icl_nlu_8296shot_balance',\n",
       " 'memalpha_icl_trec_coarse_6600shot_balance',\n",
       " 'memalpha_perltqa',\n",
       " 'memalpha_pubmed-rct',\n",
       " 'memalpha_squad'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "set(pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_dev_verl.parquet\")['data_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2965878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 140 rows from 7 sources to /mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_dev_verl_small.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_993597/476900753.py:7: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda group: group.sample(n=20, random_state=42) if len(group) >= 20 else group)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_path = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_dev_verl.parquet\"\n",
    "output_path = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_dev_verl_small.parquet\"\n",
    "df = pd.read_parquet(input_path)\n",
    "sampled = (\n",
    "    df.groupby(\"data_source\", group_keys=False)\n",
    "    .apply(lambda group: group.sample(n=20, random_state=42) if len(group) >= 20 else group)\n",
    "    .reset_index(drop=True)\n",
    " )  # sample per source or keep all if fewer than 20\n",
    "sampled.to_parquet(output_path)\n",
    "print(f\"Saved {len(sampled)} rows from {sampled['data_source'].nunique()} sources to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "974e210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 825 JSON files\n",
      "2359440\n",
      "243830\n",
      "1339\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "base_dir = Path(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/ConvoMem/ConvoMem/core_benchmark/pre_mixed_testcases\")\n",
    "json_files = sorted(p.as_posix() for p in base_dir.rglob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "lengths = []\n",
    "for path in json_files:\n",
    "    total_data = json.load(open(path))\n",
    "    for item in total_data:\n",
    "        context = '\\n'.join([mess['text'] for conv in item['conversations'] for mess in conv['messages']])\n",
    "        lengths.append(len(context))\n",
    "\n",
    "print(max(lengths))\n",
    "print(len(lengths))\n",
    "print(len([x for x in lengths if x >  1000000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f035b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ef3dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_data[0]['evidenceItems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00fc4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1242 JSON files\n",
      "40590\n",
      "75336\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "base_dir = Path(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/ConvoMem/ConvoMem/core_benchmark/evidence_questions\")\n",
    "json_files = sorted(p.as_posix() for p in base_dir.rglob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "lengths = []\n",
    "for path in json_files:\n",
    "    total_data = json.load(open(path))\n",
    "    for item in total_data['evidence_items']:\n",
    "        context = '\\n'.join([mess['text'] for conv in item['conversations'] for mess in conv['messages']])\n",
    "        lengths.append(len(context))\n",
    "\n",
    "print(max(lengths))\n",
    "print(len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89bb8d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, I'm trying to finalize my fitness schedule for the week. Can you help me out?\n",
      "Of course! What activities are you planning to include?\n",
      "Well, climbing is definitely on the agenda. It's such a great workout and stress reliever.\n",
      "Climbing sounds exciting! Do you have a specific day in mind for it?\n",
      "Okay, let's lock in my climbing for the week. I'm going to hit Crux Climbing Center on Tuesday evening.\n",
      "Tuesday evening at Crux Climbing Center it is. Anything else you'd like to add to your schedule?\n",
      "Not at the moment, but I'll let you know if anything else comes up. Thanks for the help!\n",
      "You're welcome! Let me know if you need any more assistance with your schedule.\n",
      "I've been feeling a bit worn out lately. I think I need to plan some downtime.\n",
      "It's important to listen to your body. Do you want to schedule a rest day?\n",
      "Yes, definitely. I need to make sure I schedule a full rest day. My body is telling me it needs one. Let's block out this Wednesday for recovery.\n",
      "Wednesday is now your designated rest day. It's good to have a balance between activity and rest.\n",
      "Absolutely. I always feel better after a proper rest.\n",
      "Great! Let me know if there's anything else you need to plan.\n",
      "I'm thinking about adding some cardio to my routine. What do you think?\n",
      "Cardio is a great addition to any fitness routine. Do you have a specific type of cardio in mind?\n",
      "I should probably squeeze in some light cardio. Let's pencil in a short run for Thursday morning before my meetings start.\n",
      "A short run on Thursday morning sounds perfect. It's a great way to start the day!\n",
      "Exactly, and it won't interfere with my work schedule.\n",
      "That's a smart plan. Let me know if you need help with anything else.\n",
      "I'm trying to fit in another climbing session this week. Any suggestions on when?\n",
      "How about later in the week? That way, you have some time to recover from your first session.\n",
      "For my second climbing session of the week, I want to go to Austin Bouldering Project. Friday after work seems like a good time.\n",
      "Friday after work at Austin Bouldering Project sounds like a great plan. You'll have the weekend to relax afterward.\n",
      "Exactly, and it gives me something to look forward to at the end of the week.\n",
      "That's a great way to end the week. Let me know if there's anything else you need to plan.\n",
      "The weather is supposed to be amazing this weekend. I want to do something outdoors.\n",
      "That sounds like a great idea. Do you have any specific activities in mind?\n",
      "The weather for the weekend looks amazing. I want to take Bowie for a long hike. Let's plan on going to St. Edwards Park on Saturday.\n",
      "A long hike at St. Edwards Park with Bowie on Saturday sounds perfect. It's a great way to enjoy the nice weather.\n",
      "I can't wait. Bowie will love it too!\n",
      "I'm sure he will. Let me know if you need help planning anything else.\n",
      "I want to wrap up my weekend with something relaxing. Any suggestions?\n",
      "How about a yoga class? It's a great way to relax and stretch after a busy week.\n",
      "To round out the weekend, I think a yoga class would be perfect for stretching. I'll book one for Sunday.\n",
      "A yoga class on Sunday sounds like a perfect way to end the weekend. You'll feel refreshed for the upcoming week.\n",
      "Exactly what I need. Thanks for the suggestion!\n",
      "You're welcome! Let me know if there's anything else you need help with.\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee508981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/Mem-alpha/data/memalpha/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39417ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45d8ed12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>chunks</th>\n",
       "      <th>questions_and_answers</th>\n",
       "      <th>data_source</th>\n",
       "      <th>metadata</th>\n",
       "      <th>num_chunks</th>\n",
       "      <th>num_questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>[\"[Dialogue between User and Assistant on 2024...</td>\n",
       "      <td>[{\"question\": \"Sentence: fight me in street fi...</td>\n",
       "      <td>icl_nlu_8296shot_balance</td>\n",
       "      <td>{\"data_source\": \"icl_nlu_8296shot_balance\", \"m...</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>[\"[Dialogue between User and Assistant on 2024...</td>\n",
       "      <td>[{\"question\": \"Sentence: fight me in street fi...</td>\n",
       "      <td>icl_nlu_8296shot_balance</td>\n",
       "      <td>{\"data_source\": \"icl_nlu_8296shot_balance\", \"m...</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I will provide you with sequential information...</td>\n",
       "      <td>[\"[Dialogue between User and Assistant on 2024...</td>\n",
       "      <td>[{\"question\": \"Ghost Trio, starring Ronald Pic...</td>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>{\"data_source\": \"hotpotqa\", \"metadata\": \"{}\"}</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I will provide you with sequential information...</td>\n",
       "      <td>[\"[Dialogue between User and Assistant on 2024...</td>\n",
       "      <td>[{\"question\": \"What is Ajima the deity of?\", \"...</td>\n",
       "      <td>squad</td>\n",
       "      <td>{\"data_source\": \"squad\", \"metadata\": \"{\\\"sub_s...</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I will provide you with sequential information...</td>\n",
       "      <td>[\"[Dialogue between User and Assistant on 2024...</td>\n",
       "      <td>[{\"question\": \"Were Vector and Dreams So Real ...</td>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>{\"data_source\": \"hotpotqa\", \"metadata\": \"{}\"}</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>562</td>\n",
       "      <td>I will provide you with sequential information...</td>\n",
       "      <td>[\"[Dialogue between User and Assistant on 2024...</td>\n",
       "      <td>[{\"question\": \"Who believes that the market do...</td>\n",
       "      <td>squad</td>\n",
       "      <td>{\"data_source\": \"squad\", \"metadata\": \"{\\\"sub_s...</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>563</td>\n",
       "      <td>I will provide you with sequential information...</td>\n",
       "      <td>[\"[Dialogue between User and Assistant on 2024...</td>\n",
       "      <td>[{\"question\": \"In what 2016 film did Chad Will...</td>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>{\"data_source\": \"hotpotqa\", \"metadata\": \"{}\"}</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>564</td>\n",
       "      <td>I will provide you with sequential information...</td>\n",
       "      <td>[\"[Dialogue between User and Assistant on 2024...</td>\n",
       "      <td>[{\"question\": \"In what city is the university ...</td>\n",
       "      <td>hotpotqa</td>\n",
       "      <td>{\"data_source\": \"hotpotqa\", \"metadata\": \"{}\"}</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>565</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>[\"[Dialogue between User and Assistant on 2024...</td>\n",
       "      <td>[{\"question\": \"Sentence: Adverse events occurr...</td>\n",
       "      <td>pubmed-rct</td>\n",
       "      <td>{\"data_source\": \"pubmed-rct\", \"metadata\": \"{}\"}</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>566</td>\n",
       "      <td>I will provide you with the conversation histo...</td>\n",
       "      <td>[\"[Dialogue at timestamp 2023/05/22 (Mon) 00:4...</td>\n",
       "      <td>[{\"question\": \"How long have I been working be...</td>\n",
       "      <td>lme_train</td>\n",
       "      <td>{\"data_source\": \"lme_train\", \"metadata\": \"{\\\"s...</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     instance_id                                             prompt  \\\n",
       "0              0  I will provide you with classification trainin...   \n",
       "1              1  I will provide you with classification trainin...   \n",
       "2              2  I will provide you with sequential information...   \n",
       "3              3  I will provide you with sequential information...   \n",
       "4              4  I will provide you with sequential information...   \n",
       "..           ...                                                ...   \n",
       "562          562  I will provide you with sequential information...   \n",
       "563          563  I will provide you with sequential information...   \n",
       "564          564  I will provide you with sequential information...   \n",
       "565          565  You are a scientific document classifier. I wi...   \n",
       "566          566  I will provide you with the conversation histo...   \n",
       "\n",
       "                                                chunks  \\\n",
       "0    [\"[Dialogue between User and Assistant on 2024...   \n",
       "1    [\"[Dialogue between User and Assistant on 2024...   \n",
       "2    [\"[Dialogue between User and Assistant on 2024...   \n",
       "3    [\"[Dialogue between User and Assistant on 2024...   \n",
       "4    [\"[Dialogue between User and Assistant on 2024...   \n",
       "..                                                 ...   \n",
       "562  [\"[Dialogue between User and Assistant on 2024...   \n",
       "563  [\"[Dialogue between User and Assistant on 2024...   \n",
       "564  [\"[Dialogue between User and Assistant on 2024...   \n",
       "565  [\"[Dialogue between User and Assistant on 2024...   \n",
       "566  [\"[Dialogue at timestamp 2023/05/22 (Mon) 00:4...   \n",
       "\n",
       "                                 questions_and_answers  \\\n",
       "0    [{\"question\": \"Sentence: fight me in street fi...   \n",
       "1    [{\"question\": \"Sentence: fight me in street fi...   \n",
       "2    [{\"question\": \"Ghost Trio, starring Ronald Pic...   \n",
       "3    [{\"question\": \"What is Ajima the deity of?\", \"...   \n",
       "4    [{\"question\": \"Were Vector and Dreams So Real ...   \n",
       "..                                                 ...   \n",
       "562  [{\"question\": \"Who believes that the market do...   \n",
       "563  [{\"question\": \"In what 2016 film did Chad Will...   \n",
       "564  [{\"question\": \"In what city is the university ...   \n",
       "565  [{\"question\": \"Sentence: Adverse events occurr...   \n",
       "566  [{\"question\": \"How long have I been working be...   \n",
       "\n",
       "                  data_source  \\\n",
       "0    icl_nlu_8296shot_balance   \n",
       "1    icl_nlu_8296shot_balance   \n",
       "2                    hotpotqa   \n",
       "3                       squad   \n",
       "4                    hotpotqa   \n",
       "..                        ...   \n",
       "562                     squad   \n",
       "563                  hotpotqa   \n",
       "564                  hotpotqa   \n",
       "565                pubmed-rct   \n",
       "566                 lme_train   \n",
       "\n",
       "                                              metadata  num_chunks  \\\n",
       "0    {\"data_source\": \"icl_nlu_8296shot_balance\", \"m...          10   \n",
       "1    {\"data_source\": \"icl_nlu_8296shot_balance\", \"m...          10   \n",
       "2        {\"data_source\": \"hotpotqa\", \"metadata\": \"{}\"}          11   \n",
       "3    {\"data_source\": \"squad\", \"metadata\": \"{\\\"sub_s...           5   \n",
       "4        {\"data_source\": \"hotpotqa\", \"metadata\": \"{}\"}          10   \n",
       "..                                                 ...         ...   \n",
       "562  {\"data_source\": \"squad\", \"metadata\": \"{\\\"sub_s...          10   \n",
       "563      {\"data_source\": \"hotpotqa\", \"metadata\": \"{}\"}           9   \n",
       "564      {\"data_source\": \"hotpotqa\", \"metadata\": \"{}\"}           7   \n",
       "565    {\"data_source\": \"pubmed-rct\", \"metadata\": \"{}\"}          10   \n",
       "566  {\"data_source\": \"lme_train\", \"metadata\": \"{\\\"s...          16   \n",
       "\n",
       "     num_questions  \n",
       "0              100  \n",
       "1              100  \n",
       "2               13  \n",
       "3               70  \n",
       "4               20  \n",
       "..             ...  \n",
       "562            100  \n",
       "563             21  \n",
       "564             20  \n",
       "565            100  \n",
       "566              4  \n",
       "\n",
       "[567 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0795391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hotpotqa': 100,\n",
       "         'squad': 100,\n",
       "         'booksum': 100,\n",
       "         'pubmed-rct': 90,\n",
       "         'icl_trec_coarse_6600shot_balance': 51,\n",
       "         'lme_train': 50,\n",
       "         'icl_nlu_8296shot_balance': 49,\n",
       "         'perltqa': 27})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for source in df['data_source']:\n",
    "    counter[source] += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc84df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(counter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665be89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_source</th>\n",
       "      <th>prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>reward_model</th>\n",
       "      <th>extra_info</th>\n",
       "      <th>agent_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>memalpha_icl_nlu_8296shot_balance</td>\n",
       "      <td>[{'content': ['Sentence: fight me in street fi...</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>{'ground_truth': ['43', '66', '5', '14', '15',...</td>\n",
       "      <td>{'index': 0, 'num_chunks': 10, 'original_sampl...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>memalpha_icl_nlu_8296shot_balance</td>\n",
       "      <td>[{'content': ['Sentence: what do you want to d...</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>{'ground_truth': ['66', '38', '5', '5', '5', '...</td>\n",
       "      <td>{'index': 1, 'num_chunks': 10, 'original_sampl...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>memalpha_icl_nlu_8296shot_balance</td>\n",
       "      <td>[{'content': ['Sentence: what are the bbc poll...</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>{'ground_truth': ['36', '38', '14', '18', '65'...</td>\n",
       "      <td>{'index': 2, 'num_chunks': 10, 'original_sampl...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>memalpha_icl_nlu_8296shot_balance</td>\n",
       "      <td>[{'content': ['Sentence: add eggs to my grocer...</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>{'ground_truth': ['57', '66', '21', '12', '29'...</td>\n",
       "      <td>{'index': 3, 'num_chunks': 10, 'original_sampl...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>memalpha_icl_nlu_8296shot_balance</td>\n",
       "      <td>[{'content': ['Sentence: send an email to emma...</td>\n",
       "      <td>I will provide you with classification trainin...</td>\n",
       "      <td>{'ground_truth': ['50', '36', '23', '17', '41'...</td>\n",
       "      <td>{'index': 4, 'num_chunks': 10, 'original_sampl...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>memalpha_pubmed-rct</td>\n",
       "      <td>[{'content': ['Sentence: It appears that poten...</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>{'ground_truth': ['1', '4', '2', '3', '4', '0'...</td>\n",
       "      <td>{'index': 3429, 'num_chunks': 10, 'original_sa...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3430</th>\n",
       "      <td>memalpha_pubmed-rct</td>\n",
       "      <td>[{'content': ['Sentence: Oxygen extraction rat...</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>{'ground_truth': ['4', '0', '2', '4', '2', '4'...</td>\n",
       "      <td>{'index': 3430, 'num_chunks': 10, 'original_sa...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3431</th>\n",
       "      <td>memalpha_pubmed-rct</td>\n",
       "      <td>[{'content': ['Sentence: The differences betwe...</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>{'ground_truth': ['4', '3', '4', '2', '0', '2'...</td>\n",
       "      <td>{'index': 3431, 'num_chunks': 10, 'original_sa...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3432</th>\n",
       "      <td>memalpha_pubmed-rct</td>\n",
       "      <td>[{'content': ['Sentence: We studied 137 patien...</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>{'ground_truth': ['2', '1', '0', '2', '1', '4'...</td>\n",
       "      <td>{'index': 3432, 'num_chunks': 10, 'original_sa...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>memalpha_pubmed-rct</td>\n",
       "      <td>[{'content': ['Sentence: Invasive fungal infec...</td>\n",
       "      <td>You are a scientific document classifier. I wi...</td>\n",
       "      <td>{'ground_truth': ['3', '4', '2', '4', '4', '0'...</td>\n",
       "      <td>{'index': 3433, 'num_chunks': 10, 'original_sa...</td>\n",
       "      <td>tool_mem_agent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3434 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            data_source  \\\n",
       "0     memalpha_icl_nlu_8296shot_balance   \n",
       "1     memalpha_icl_nlu_8296shot_balance   \n",
       "2     memalpha_icl_nlu_8296shot_balance   \n",
       "3     memalpha_icl_nlu_8296shot_balance   \n",
       "4     memalpha_icl_nlu_8296shot_balance   \n",
       "...                                 ...   \n",
       "3429                memalpha_pubmed-rct   \n",
       "3430                memalpha_pubmed-rct   \n",
       "3431                memalpha_pubmed-rct   \n",
       "3432                memalpha_pubmed-rct   \n",
       "3433                memalpha_pubmed-rct   \n",
       "\n",
       "                                                 prompt  \\\n",
       "0     [{'content': ['Sentence: fight me in street fi...   \n",
       "1     [{'content': ['Sentence: what do you want to d...   \n",
       "2     [{'content': ['Sentence: what are the bbc poll...   \n",
       "3     [{'content': ['Sentence: add eggs to my grocer...   \n",
       "4     [{'content': ['Sentence: send an email to emma...   \n",
       "...                                                 ...   \n",
       "3429  [{'content': ['Sentence: It appears that poten...   \n",
       "3430  [{'content': ['Sentence: Oxygen extraction rat...   \n",
       "3431  [{'content': ['Sentence: The differences betwe...   \n",
       "3432  [{'content': ['Sentence: We studied 137 patien...   \n",
       "3433  [{'content': ['Sentence: Invasive fungal infec...   \n",
       "\n",
       "                                                context  \\\n",
       "0     I will provide you with classification trainin...   \n",
       "1     I will provide you with classification trainin...   \n",
       "2     I will provide you with classification trainin...   \n",
       "3     I will provide you with classification trainin...   \n",
       "4     I will provide you with classification trainin...   \n",
       "...                                                 ...   \n",
       "3429  You are a scientific document classifier. I wi...   \n",
       "3430  You are a scientific document classifier. I wi...   \n",
       "3431  You are a scientific document classifier. I wi...   \n",
       "3432  You are a scientific document classifier. I wi...   \n",
       "3433  You are a scientific document classifier. I wi...   \n",
       "\n",
       "                                           reward_model  \\\n",
       "0     {'ground_truth': ['43', '66', '5', '14', '15',...   \n",
       "1     {'ground_truth': ['66', '38', '5', '5', '5', '...   \n",
       "2     {'ground_truth': ['36', '38', '14', '18', '65'...   \n",
       "3     {'ground_truth': ['57', '66', '21', '12', '29'...   \n",
       "4     {'ground_truth': ['50', '36', '23', '17', '41'...   \n",
       "...                                                 ...   \n",
       "3429  {'ground_truth': ['1', '4', '2', '3', '4', '0'...   \n",
       "3430  {'ground_truth': ['4', '0', '2', '4', '2', '4'...   \n",
       "3431  {'ground_truth': ['4', '3', '4', '2', '0', '2'...   \n",
       "3432  {'ground_truth': ['2', '1', '0', '2', '1', '4'...   \n",
       "3433  {'ground_truth': ['3', '4', '2', '4', '4', '0'...   \n",
       "\n",
       "                                             extra_info      agent_name  \n",
       "0     {'index': 0, 'num_chunks': 10, 'original_sampl...  tool_mem_agent  \n",
       "1     {'index': 1, 'num_chunks': 10, 'original_sampl...  tool_mem_agent  \n",
       "2     {'index': 2, 'num_chunks': 10, 'original_sampl...  tool_mem_agent  \n",
       "3     {'index': 3, 'num_chunks': 10, 'original_sampl...  tool_mem_agent  \n",
       "4     {'index': 4, 'num_chunks': 10, 'original_sampl...  tool_mem_agent  \n",
       "...                                                 ...             ...  \n",
       "3429  {'index': 3429, 'num_chunks': 10, 'original_sa...  tool_mem_agent  \n",
       "3430  {'index': 3430, 'num_chunks': 10, 'original_sa...  tool_mem_agent  \n",
       "3431  {'index': 3431, 'num_chunks': 10, 'original_sa...  tool_mem_agent  \n",
       "3432  {'index': 3432, 'num_chunks': 10, 'original_sa...  tool_mem_agent  \n",
       "3433  {'index': 3433, 'num_chunks': 10, 'original_sa...  tool_mem_agent  \n",
       "\n",
       "[3434 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca22271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'memalpha_squad': 957,\n",
       "         'memalpha_pubmed-rct': 900,\n",
       "         'memalpha_icl_trec_coarse_6600shot_balance': 510,\n",
       "         'memalpha_icl_nlu_8296shot_balance': 490,\n",
       "         'memalpha_perltqa': 270,\n",
       "         'memalpha_hotpotqa': 207,\n",
       "         'memalpha_booksum': 100})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for source in df['data_source']:\n",
    "    counter[source] += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e224c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhangkehao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
