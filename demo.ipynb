{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_dev.parquet\"\n",
    "df = pd.read_parquet(file_path)\n",
    "df['agent_name'] = 'mem_agent'\n",
    "df.to_parquet(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_dev_agent_loop.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2bdbcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据条数：500\n",
      "正确率：0.382\n",
      "knowledge-update 0.4358974358974359\n",
      "34 / 78\n",
      "single-session-user 0.4142857142857143\n",
      "29 / 70\n",
      "temporal-reasoning 0.46616541353383456\n",
      "62 / 133\n",
      "single-session-preference 0.6\n",
      "18 / 30\n",
      "single-session-assistant 0.17857142857142858\n",
      "10 / 56\n",
      "multi-session 0.2857142857142857\n",
      "38 / 133\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/HippoRAG/longmemeval_s_r1_log_no_longcontext_recall20_eval_res.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/longmemeval_s/Qwen3-8B-5k-1k-infty_eval_res.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/processing_log_qwen3-8b_1758104883.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/processing_log_qwen3-8b_faster_1758253073.jsonl\"\n",
    "# datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/results/qwen3-8b/longmemeval/evaluated_concat.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/results/qwen3-8b/longmemeval_oracle/evaluated_concat.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/ReMe/cookbook/simple_demo/longmemeval/evaluated_unknown.jsonl\"\n",
    "data = [json.loads(line) for line in open(datapath)]\n",
    "\n",
    "original_datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/data/longmemeval_s.json\"\n",
    "original_data = json.load(open(original_datapath))\n",
    "for d in data:\n",
    "    for od in original_data:\n",
    "        question_id = d.get(\"question_id\", d.get(\"qid\"))\n",
    "        if \"longmemeval_\" in question_id:\n",
    "            question_id = question_id[len(\"longmemeval_\"):-2]\n",
    "        if od[\"question_id\"] == question_id:\n",
    "            d[\"question_type\"] = od[\"question_type\"]\n",
    "            break\n",
    "\n",
    "# datapath = \"longmemeval_s_r1_log_no_longcontext_recall20.json\"\n",
    "# data = json.load(open(datapath))\n",
    "print(\"数据条数：\" + str(len([x for x in data if x.get('qid') is not None])))\n",
    "print(\"正确率：\" + str(len([x for x in data if x[\"metric\"].get('llm_score')]) / len([x for x in data if x[\"metric\"].get('llm_score') is not None])))\n",
    "\n",
    "for tp in [\"knowledge-update\", \"single-session-user\", \"temporal-reasoning\", \"single-session-preference\", \"single-session-assistant\", \"multi-session\"]:\n",
    "    if len([x for x in data if x.get('question_type')==tp]) == 0:\n",
    "        print(tp, 0)\n",
    "    else:\n",
    "        print(tp, len([x for x in data if x[\"metric\"].get('llm_score') and x.get('question_type')==tp]) / len([x for x in data if x.get('question_type')==tp]))\n",
    "        print(f\"{len([x for x in data if x[\"metric\"].get('llm_score') and x.get('question_type')==tp])} / {len([x for x in data if x.get('question_type')==tp])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e74531ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据条数：1986\n",
      "正确率：0.28751258811681774\n",
      "multi hop 0.09574468085106383\n",
      "27 / 282\n",
      "temporal 0.07476635514018691\n",
      "24 / 321\n",
      "open domain 0.21875\n",
      "21 / 96\n",
      "single hop 0.06539833531510107\n",
      "55 / 841\n",
      "adversarial 0.9955156950672646\n",
      "444 / 446\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/results/qwen3-8b/locomo/evaluated_concat.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/locomo_results_qwen3-8b_faster_1759028934.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/emergence_simple_fast/locomo_results_1759036272.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/locomo/Qwen3-8B-5k-1k-infty.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/HippoRAG/locomo_hippo_qwen_recall42.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/HippoRAG/locomo_hippo_qwen_recall20.jsonl\"\n",
    "datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/ReMe/cookbook/simple_demo/locomo/evaluated_unknown.jsonl\"\n",
    "data = [json.loads(line) for line in open(datapath)]\n",
    "# data = sum([d[\"queries_solutions\"] for d in data], start=[])\n",
    "\n",
    "original_datapath = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/raw/locomo10.json\"\n",
    "original_data = json.load(open(original_datapath))\n",
    "\n",
    "for d in data:\n",
    "    for od in original_data:\n",
    "        if od['sample_id'] in d['qid']:\n",
    "            q_index = int(d['qid'].split('_')[2])\n",
    "            d['question_type'] = od['qa'][q_index]['category']\n",
    "\n",
    "for d in data:\n",
    "    # d['question_type'] = d['category']\n",
    "    d[\"gpt4o_score\"] =  d[\"metric\"].get('llm_score')\n",
    "\n",
    "print(\"数据条数：\" + str(len(data)))\n",
    "print(\"正确率：\" + str(len([x for x in data if x[\"gpt4o_score\"]]) / len([x for x in data if x[\"gpt4o_score\"] is not None])))\n",
    "\n",
    "question_type_map = {\n",
    "    1: \"multi hop\",\n",
    "    2: \"temporal\",\n",
    "    3: \"open domain\",\n",
    "    4: \"single hop\",\n",
    "    5: \"adversarial\"\n",
    "}\n",
    "\n",
    "\n",
    "for tp in range(1,6):\n",
    "    if len([x for x in data if x.get('question_type')==tp]) == 0:\n",
    "        print(question_type_map[tp], 0)\n",
    "    else:\n",
    "        print(question_type_map[tp], len([x for x in data if x[\"gpt4o_score\"] and x.get('question_type')==tp]) / len([x for x in data if x.get('question_type')==tp]))\n",
    "        print(f\"{len([x for x in data if x[\"gpt4o_score\"] and x.get('question_type')==tp])} / {len([x for x in data if x.get('question_type')==tp])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "from openai import OpenAI, RateLimitError\n",
    "import uuid\n",
    "import backoff\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm as atqdm\n",
    "\n",
    "from config import MODEL_NAME, API_CONFIG_LOCAL\n",
    "from openai import OpenAI\n",
    "\n",
    "async def gpt4o_evaluation(question, correct_answer, model_response, question_type):\n",
    "    \"\"\"\n",
    "    Use GPT-4o to evaluate the model response\n",
    "    Based on reprocess_res.py\n",
    "    \"\"\"\n",
    "\n",
    "    API_CONFIG_LOCAL[\"base_url\"] = \"http://172.24.139.15:8000/v1\"\n",
    "    openai_client = OpenAI(**API_CONFIG_LOCAL)\n",
    "\n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        RateLimitError,\n",
    "        max_tries=16,\n",
    "        max_time=300,\n",
    "        jitter=backoff.full_jitter\n",
    "    )\n",
    "    async def llm_response(messages):\n",
    "        # 使用 asyncio.to_thread 将同步调用转为异步\n",
    "        res = await asyncio.to_thread(\n",
    "            openai_client.chat.completions.create,\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "        response_message = res.choices[0].message.content\n",
    "        if \"</think>\" in response_message:\n",
    "            response_message = response_message.split(\"</think>\")[-1].strip()\n",
    "        # print(response_message)\n",
    "        return response_message\n",
    "    \n",
    "    def get_anscheck_prompt(task, question, answer, response):\n",
    "        LLM_TEMPLATE = \"I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no.\\n\\nQuestion: {}\\n\\nCorrect Answer: {}\\n\\nModel Response: {}\\n\\nIs the model response correct? Answer yes or no only.\"\n",
    "        prompt = LLM_TEMPLATE.format(question, answer, response)\n",
    "        return prompt\n",
    "    \n",
    "    async def anscheck(task, question, answer, response):\n",
    "        prompt = get_anscheck_prompt(task, question, answer, response)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        res = await llm_response(messages)\n",
    "        return 'yes' in res.lower() and not 'no' in res.lower()\n",
    "    \n",
    "    return await anscheck(question_type, question, correct_answer, model_response)\n",
    "\n",
    "async def process_item(d, output_file, semaphore, file_lock):\n",
    "    async with semaphore:\n",
    "        score = await gpt4o_evaluation(d[\"input\"], d[\"answer\"], d[\"response\"], d[\"question_type\"])\n",
    "        d[\"gpt4o_score\"] = score\n",
    "        \n",
    "        # 使用文件锁确保并发写入安全\n",
    "        async with file_lock:\n",
    "            with open(output_file, \"a\") as f:\n",
    "                f.write(json.dumps(d) + '\\n')\n",
    "        \n",
    "        return d\n",
    "\n",
    "async def main():\n",
    "    data_path = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/locomo/Qwen3-8B-5k-1k-infty.jsonl\"\n",
    "    data = [json.loads(line) for line in open(data_path)]\n",
    "    \n",
    "    output_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_eval/results/locomo/Qwen3-8B-5k-1k-infty_.jsonl\"\n",
    "    \n",
    "    # 创建信号量控制并发数\n",
    "    semaphore = asyncio.Semaphore(20)  # 可调整并发数\n",
    "    \n",
    "    # 创建文件锁，确保并发写入安全\n",
    "    file_lock = asyncio.Lock()\n",
    "    \n",
    "    # 创建所有任务\n",
    "    tasks = [process_item(d, output_file, semaphore, file_lock) for d in data]\n",
    "    \n",
    "    # 使用异步进度条执行\n",
    "    results = await atqdm.gather(*tasks, desc=\"Processing\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # asyncio.run(main())\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348906d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id=None, choices=None, created=None, model=None, object=None, service_tier=None, system_fingerprint=None, usage=None, code=100049, msg='接口验证失败,请检查Token是否正确', success=False, data={})\n"
     ]
    }
   ],
   "source": [
    "from config import MODEL_NAME, API_CONFIG_LOCAL, API_CONFIG\n",
    "from openai import OpenAI\n",
    "API_CONFIG_LOCAL[\"base_url\"] = \"http://172.24.139.15:8000/v1\"\n",
    "openai_client = OpenAI(**API_CONFIG_LOCAL)\n",
    "openai_client = OpenAI(**API_CONFIG)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hello\"\n",
    "    }\n",
    "]\n",
    "res = openai_client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    max_tokens=2048,\n",
    "    logprobs=1\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc7d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<think>',\n",
       " '\\n',\n",
       " 'Okay',\n",
       " ',',\n",
       " ' the',\n",
       " ' user',\n",
       " ' said',\n",
       " ' \"',\n",
       " 'hello',\n",
       " '\".',\n",
       " ' I',\n",
       " ' need',\n",
       " ' to',\n",
       " ' respond',\n",
       " ' appropriately',\n",
       " '.',\n",
       " ' Since',\n",
       " ' it',\n",
       " \"'s\",\n",
       " ' a',\n",
       " ' greeting',\n",
       " ',',\n",
       " ' I',\n",
       " ' should',\n",
       " ' acknowledge',\n",
       " ' it',\n",
       " ' and',\n",
       " ' offer',\n",
       " ' assistance',\n",
       " '.',\n",
       " ' Let',\n",
       " ' me',\n",
       " ' make',\n",
       " ' sure',\n",
       " ' the',\n",
       " ' response',\n",
       " ' is',\n",
       " ' friendly',\n",
       " ' and',\n",
       " ' open',\n",
       " '-ended',\n",
       " '.',\n",
       " ' Maybe',\n",
       " ' ask',\n",
       " ' how',\n",
       " ' I',\n",
       " ' can',\n",
       " ' help',\n",
       " ' them',\n",
       " ' today',\n",
       " '.',\n",
       " ' Keep',\n",
       " ' it',\n",
       " ' simple',\n",
       " ' and',\n",
       " ' welcoming',\n",
       " '.\\n',\n",
       " '</think>',\n",
       " '\\n\\n',\n",
       " 'Hello',\n",
       " '!',\n",
       " ' How',\n",
       " ' can',\n",
       " ' I',\n",
       " ' assist',\n",
       " ' you',\n",
       " ' today',\n",
       " '?',\n",
       " ' �',\n",
       " '�',\n",
       " '<|im_end|>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.token for x in res.choices[0].logprobs.content]\n",
    "res.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5574cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_context() -> str:\n",
    "    \"\"\"Load contextual information fed into ``ToolMemoryAgentLoop``.\n",
    "\n",
    "    Projects can override this helper to supply domain-specific context. By default it\n",
    "    returns the value from ``TOOL_MEM_AGENT_TEST_CONTEXT`` (or an empty string).\n",
    "    \"\"\"\n",
    "\n",
    "    # return os.environ.get(\"TOOL_MEM_AGENT_TEST_CONTEXT\", \"I like to eat bananas.\")\n",
    "    raw = json.load(open(\"/mnt/pfs-guan-ssai/nlu/zhangkehao/zep/benchmarks/longmemeval/data/longmemeval_s.json\"))\n",
    "    item0 = raw[0]\n",
    "    context = \"Below is a conversation between User and Assistant.\\n\\n\"\n",
    "    for idx, session in enumerate(item0[\"haystack_sessions\"]):\n",
    "        session_date = item0[\"haystack_dates\"][idx]\n",
    "        session_conv = ''\n",
    "        for turn in session:\n",
    "            role = \"User\" if turn['role'] == 'user' else \"Assistant\"\n",
    "            turn_text = f'{role} said, \"{turn[\"content\"]}\"'\n",
    "            turn_text += '\\n'\n",
    "            session_conv += turn_text\n",
    "        if not session_conv:\n",
    "            session_conv = \"NO CONVERSATION\"\n",
    "        query_conv = f'DATE: {session_date}\\nCONVERSATION:\\n{session_conv}\\n\\n'\n",
    "        context += query_conv\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e5311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat\n",
    "import json\n",
    "input_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/verl/base_result.jsonl\"\n",
    "output_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/verl/base_result_reformat.jsonl\"\n",
    "original_file = \"/mnt/pfs-guan-ssai/nlu/zhangkehao/zep/benchmarks/longmemeval/data/longmemeval_s.json\"\n",
    "data = [json.loads(line) for line in open(input_file)]\n",
    "original_data = json.load(open(original_file))\n",
    "with open(output_file, \"w\") as f:\n",
    "    for d in data:\n",
    "        original_d = original_data[d['idx']]\n",
    "        response = d[\"response\"]\n",
    "        if \"\\\\boxed{\" in response and \"}\" in response.split(\"\\\\boxed{\")[-1]:\n",
    "            response = response.split(\"\\\\boxed{\")[-1]\n",
    "            response = response[:response.rindex(\"}\")]\n",
    "        new_d = {\n",
    "            \"qid\": \"longmemeval_\" + original_d[\"question_id\"] + \"_0\",\n",
    "            \"query\": original_d[\"question\"],\n",
    "            \"expected_answer\": original_d[\"answer\"],\n",
    "            \"response\": response\n",
    "        }\n",
    "        f.write(json.dumps(new_d) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94d4a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memalpha\n",
      "9.89596167008898 chunks\n",
      "4046.8266011896526 chars\n",
      "\n",
      "locomo\n",
      "19.0 chunks\n",
      "3942.8947368421054 chars\n",
      "\n",
      "hotpotqa\n",
      "61.06103515625 chunks\n",
      "1701.0528355977065 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "memalpha_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/memalpha_dev_verl.parquet'\n",
    "locomo_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/locomo_train_verl.parquet'\n",
    "hpq_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/MemAgent_minimal/taskutils/memory_data/hotpotqa_train_mem_agent_loop_chunk2000.parquet'\n",
    "for input_file, name in zip([memalpha_file, locomo_file, hpq_file], ['memalpha', 'locomo', 'hotpotqa']):\n",
    "    print(name)\n",
    "    df = pd.read_parquet(input_file)\n",
    "    print(np.mean([len([len(x) for x in y['tools_kwargs']['memory_bm25_retrieve']['create_kwargs']['chunks']]) for y in df['extra_info']]), 'chunks') # 平均chunks数量\n",
    "    print(np.mean([len(x) for y in df['extra_info'] for x in y['tools_kwargs']['memory_bm25_retrieve']['create_kwargs']['chunks']]), 'chars\\n') # 平均每个chunk字符数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18ef485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotpotqa\n",
      "200.0 chunks\n",
      "570.0202734375 chars\n",
      "\n",
      "locomo\n",
      "27.2 chunks\n",
      "3398.1507352941176 chars\n",
      "\n",
      "longmemeval\n",
      "47.764 chunks\n",
      "10521.834394104346 chars\n",
      "\n",
      "msc\n",
      "25.0 chunks\n",
      "1563.9144 chars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "hotpotqa_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_hotpotqa_200.json'\n",
    "locomo_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_locomo.json'\n",
    "longmemeval_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_longmemeval.json'\n",
    "msc_file = '/mnt/pfs-guan-ssai/nlu/zhangkehao/unified-memory-agent/data/processed_msc_batch5.json'\n",
    "for input_file, name in zip([hotpotqa_file, locomo_file, longmemeval_file, msc_file], ['hotpotqa', 'locomo', 'longmemeval', 'msc']):\n",
    "    print(name)\n",
    "    data = json.load(open(input_file))\n",
    "    print(np.mean([len(item['chunks']) for item in data]), 'chunks') # 平均chunks数量\n",
    "    print(np.mean([len(chunk) for item in data for chunk in item['chunks']]), 'chars\\n') # 平均每个chunk字符数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4127e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value(row):\n",
    "    row['extra_info']['num_chunks'] = row['extra_info'].pop('num_docs')\n",
    "    row['extra_info']['question'] = [row['extra_info']['question']]\n",
    "    return row\n",
    "df = df.apply(calculate_value, axis=1)\n",
    "df.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e10a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agent_name'] = 'tool_mem_agent'\n",
    "def get_chunks(context_text, chunk_size):\n",
    "    separator = \"\\n\"\n",
    "    small_chunks = context_text.split(separator)\n",
    "    chunks = []\n",
    "    to_append = \"\"\n",
    "    for chunk in small_chunks:\n",
    "        if len(to_append) + len(chunk) + len(separator) <= chunk_size:\n",
    "            if to_append:\n",
    "                to_append += separator + chunk\n",
    "            else:\n",
    "                to_append = chunk\n",
    "        else:\n",
    "            if to_append:\n",
    "                chunks.append(to_append)\n",
    "            to_append = chunk\n",
    "    if to_append:\n",
    "        chunks.append(to_append)\n",
    "    return chunks\n",
    "\n",
    "def calculate_value(row):\n",
    "    idx = row['extra_info']['index']\n",
    "    filename = f\"/mnt/pfs-guan-ssai/nlu/zhangkehao/verl/memagent/store/memory_store_val{idx}.jsonl\"\n",
    "    chunks = get_chunks(row['context'], 2000)\n",
    "    row['extra_info'][\"tools_kwargs\"] = {\n",
    "        \"memory_add\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_update\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_delete\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_key_retrieve\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_list\": {\n",
    "            \"create_kwargs\": {\"filename\": filename}\n",
    "        },\n",
    "        \"memory_bm25_retrieve\": {\n",
    "            \"create_kwargs\": {\"chunks\": chunks}\n",
    "        },\n",
    "        \"memory_embedding_retrieve\": {\n",
    "            \"create_kwargs\": {\"chunks\": chunks}\n",
    "        },\n",
    "    }\n",
    "    return row['extra_info']\n",
    "\n",
    "df['extra_info'] = df.apply(calculate_value, axis=1)\n",
    "df.to_parquet(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhangkehao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
